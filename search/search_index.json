{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About \u00b6 Abacus is a repository of open and licensed data hosted by UBC Library and used by the academic communities at SFU, UBC, UNBC, and UVic. The repository contains more than 2,100 datasets with significant collections of geospatial data, linguistics data, and survey microdata. The Abacus collection includes both open and licensed data: - Open data may be downloaded by anyone who visits the Abacus website . - Licensed data is available to faculty, staff, and students who login with their university credentials.","title":"About Abacus"},{"location":"#about","text":"Abacus is a repository of open and licensed data hosted by UBC Library and used by the academic communities at SFU, UBC, UNBC, and UVic. The repository contains more than 2,100 datasets with significant collections of geospatial data, linguistics data, and survey microdata. The Abacus collection includes both open and licensed data: - Open data may be downloaded by anyone who visits the Abacus website . - Licensed data is available to faculty, staff, and students who login with their university credentials.","title":"About"},{"location":"metadata-guide/","text":"Metadata guide \u00b6 This guide outlines the metadata expectations and standards for describing datasets in the Abacus data repository . Abacus is hosted on the Dataverse platform and datasets are described using the Data Documentation Initiative DDI Codebook 2.5 metadata standard. Definitions in this guide are copied from DDI 2.5 with additional notes and explanations that apply to the Abacus context. Required metadata fields \u00b6 The table below includes all the metadata fields required by the Dataverse platform. In practice most Abacus datasets have additional metadata to improve description and discovery, but only these fields are enforced by the Abacus interface. Dataverse field label DDI-C 2.5 element Title <titl> Author name <AuthEnty Contact e-mail <contact> Description text <abstract> Subject <subect> Place Contact email/contact name Unique file name (for files in a dataset) Recommendations for non-required fields \u00b6 If there are sections that should be deleted, or recommendations. Also a place to note that you need to save the 'basic block' first before you can access the other metadata fields. Added fields, after creating the dataset \u00b6 These should be added when available to improve findability and consistency across Abacus datasets. Dataverse field label | DDI-C 2.5 element | --- | --- | Production Date | <prodDate> | Geographic Coverage, Country/recommendations | <nation> | Geographic Unit | <geogUnit> | Keyword | <keyword>","title":"Metadata guide"},{"location":"metadata-guide/#metadata-guide","text":"This guide outlines the metadata expectations and standards for describing datasets in the Abacus data repository . Abacus is hosted on the Dataverse platform and datasets are described using the Data Documentation Initiative DDI Codebook 2.5 metadata standard. Definitions in this guide are copied from DDI 2.5 with additional notes and explanations that apply to the Abacus context.","title":"Metadata guide"},{"location":"metadata-guide/#required-metadata-fields","text":"The table below includes all the metadata fields required by the Dataverse platform. In practice most Abacus datasets have additional metadata to improve description and discovery, but only these fields are enforced by the Abacus interface. Dataverse field label DDI-C 2.5 element Title <titl> Author name <AuthEnty Contact e-mail <contact> Description text <abstract> Subject <subect> Place Contact email/contact name Unique file name (for files in a dataset)","title":"Required metadata fields"},{"location":"metadata-guide/#recommendations-for-non-required-fields","text":"If there are sections that should be deleted, or recommendations. Also a place to note that you need to save the 'basic block' first before you can access the other metadata fields.","title":"Recommendations for non-required fields"},{"location":"metadata-guide/#added-fields-after-creating-the-dataset","text":"These should be added when available to improve findability and consistency across Abacus datasets. Dataverse field label | DDI-C 2.5 element | --- | --- | Production Date | <prodDate> | Geographic Coverage, Country/recommendations | <nation> | Geographic Unit | <geogUnit> | Keyword | <keyword>","title":"Added fields, after creating the dataset"},{"location":"privacy/","text":"Abacus privacy policy \u00b6 Abacus creates user accounts that include your name and email address. This personal information is collected under the authority of section 26(c) of the Freedom of Information and Protection of Privacy Act (FIPPA). Your personal information is only used for authentication, to comply with data licensing terms, and will be kept confidential by UBC. Questions about the collection of this information may be directed to abacus.support@ubc.ca.","title":"Privacy policy"},{"location":"privacy/#abacus-privacy-policy","text":"Abacus creates user accounts that include your name and email address. This personal information is collected under the authority of section 26(c) of the Freedom of Information and Protection of Privacy Act (FIPPA). Your personal information is only used for authentication, to comply with data licensing terms, and will be kept confidential by UBC. Questions about the collection of this information may be directed to abacus.support@ubc.ca.","title":"Abacus privacy policy"},{"location":"dataset_code/","text":"Dataset code \u00b6 While Abacus contains the licensed data sets for you to work with, that is, the material provided by the publisher, what it doesn't contain is code written by the library and other researchers to work with this data. The Dataset code section provides scripts, tools, and guidance for working with specific datasets in Abacus. You'll also find software tools designed to work with the Abacus platform itself, and generalized data tools not specific to one particular data set. Material in this section was created by UBC Library or contributed by the research community. If you've written code, queries, or other material that may help other researchers use an Abacus dataset please consider sharing it (contact Jeremy Buhler, Data Librarian, UBC Library at jeremy.buhler@ubc.ca ). Abacus Git repository \u00b6 Computer code, unlike books or even licensed data sets, is notoriously changeable. Browser versions can change frequently; the same can be said for utilities used to analyse data. Because of this, software is often stored on systems which allow version control . More than just indicating the most current version of a piece of software, version control systems allow users to track changes in the software, allowing you to move not only forward and back over versions, but laterally over branched changes. Utilities and code snippets for material in Abacus are stored in such a system, specfically using a version control system called Git . This is more suitable than storing them within Abacus itself, as the Git system is much more user-friendly for copying, modifying and reusing software than the built-in version control system in Abacus. Take me to the Abacus git utility repository How to use Git \u00b6 There are two possible ways to use Git: from the command line, or using a graphical user interface. There is a (very) basic tutorial available for the command line interface at the [https://git-scm.com/docs/gittutorial] (Git website). There is also no shortage of tutorials vaialble simpy by searching for \"Git tutorial\". Graphical user interfaces will vary by designer and users will undoubtedly have opinions on which is better or worse. Searching for \"Best Git GUI\" will produce pages of users arguing on internet forums. Git has an \"official\" client, Github Desktop . If you are completely new to the concept and prefer something hands on and less technically dry, there is a free game which teaches the basics of Git: Oh My Git . General purpose utilities \u00b6 These tools or utilities are not specific to a single dataset \u2014 or even to the Abacus platform \u2014 but they may be useful when working with datasets from Abacus. fcheck/damage \u00b6 https://github.com/ubc-library-rc/fcheck Damage is a simple command-line utility which outputs a file manifest in a variety of formats, with a special focus on statistical package files from SPSS, SAS and Stata. FCheck is the Python 3 module which powers the damage utility, which you can use in your own python code. dataverse_utils \u00b6 https://github.com/ubc-library-rc/dataverse_utils Abacus runs on the open source Dataverse platform. This is a Python library and generalized set of utilities which help with managing Dataverse repositories. Dataset-specific resources \u00b6 Use the menu or the links below to access resources and code related to specific datasets or types of data in Abacus BC Assessment data Postal Code Conversion Files (PCCF)","title":"Dataset code"},{"location":"dataset_code/#dataset-code","text":"While Abacus contains the licensed data sets for you to work with, that is, the material provided by the publisher, what it doesn't contain is code written by the library and other researchers to work with this data. The Dataset code section provides scripts, tools, and guidance for working with specific datasets in Abacus. You'll also find software tools designed to work with the Abacus platform itself, and generalized data tools not specific to one particular data set. Material in this section was created by UBC Library or contributed by the research community. If you've written code, queries, or other material that may help other researchers use an Abacus dataset please consider sharing it (contact Jeremy Buhler, Data Librarian, UBC Library at jeremy.buhler@ubc.ca ).","title":"Dataset code"},{"location":"dataset_code/#abacus-git-repository","text":"Computer code, unlike books or even licensed data sets, is notoriously changeable. Browser versions can change frequently; the same can be said for utilities used to analyse data. Because of this, software is often stored on systems which allow version control . More than just indicating the most current version of a piece of software, version control systems allow users to track changes in the software, allowing you to move not only forward and back over versions, but laterally over branched changes. Utilities and code snippets for material in Abacus are stored in such a system, specfically using a version control system called Git . This is more suitable than storing them within Abacus itself, as the Git system is much more user-friendly for copying, modifying and reusing software than the built-in version control system in Abacus. Take me to the Abacus git utility repository","title":"Abacus Git repository"},{"location":"dataset_code/#how-to-use-git","text":"There are two possible ways to use Git: from the command line, or using a graphical user interface. There is a (very) basic tutorial available for the command line interface at the [https://git-scm.com/docs/gittutorial] (Git website). There is also no shortage of tutorials vaialble simpy by searching for \"Git tutorial\". Graphical user interfaces will vary by designer and users will undoubtedly have opinions on which is better or worse. Searching for \"Best Git GUI\" will produce pages of users arguing on internet forums. Git has an \"official\" client, Github Desktop . If you are completely new to the concept and prefer something hands on and less technically dry, there is a free game which teaches the basics of Git: Oh My Git .","title":"How to use Git"},{"location":"dataset_code/#general-purpose-utilities","text":"These tools or utilities are not specific to a single dataset \u2014 or even to the Abacus platform \u2014 but they may be useful when working with datasets from Abacus.","title":"General purpose utilities"},{"location":"dataset_code/#fcheckdamage","text":"https://github.com/ubc-library-rc/fcheck Damage is a simple command-line utility which outputs a file manifest in a variety of formats, with a special focus on statistical package files from SPSS, SAS and Stata. FCheck is the Python 3 module which powers the damage utility, which you can use in your own python code.","title":"fcheck/damage"},{"location":"dataset_code/#dataverse_utils","text":"https://github.com/ubc-library-rc/dataverse_utils Abacus runs on the open source Dataverse platform. This is a Python library and generalized set of utilities which help with managing Dataverse repositories.","title":"dataverse_utils"},{"location":"dataset_code/#dataset-specific-resources","text":"Use the menu or the links below to access resources and code related to specific datasets or types of data in Abacus BC Assessment data Postal Code Conversion Files (PCCF)","title":"Dataset-specific resources"},{"location":"dataset_code/bc_assessment/bc_assessment/","text":"Dataset name BC Assessment Data Advice and Inventory Extracts Permanent URL https://hdl.handle.net/11272.1/AB2/LAPUAB Data access rules Restricted to researchers currently affiliated with UBC Info The BC Assessment data in Abacus is currently restricted to UBC users, but other institutions may have access to the data in other forms. Contact your university library for more information. Data format and conversion \u00b6 BC Assessment provides data in XML and TXT format. To make the large XML files more accessible to researchers, UBC Library converts each to a series of CSV files and a SQLite database. Both the original files and the transformed versions are available in Abacus. The SQLite version is recommended for most users. For those interested in the file conversion, the process to create the SQLite database from the original XML and TXT files is outlined in these files: BCA_data_transformation_notes.txt describes the steps involved BCA_REVD_stylesheet.xsl is the XSLT stylesheet used in the transformation Using the sqlite database \u00b6 The open source DB Browser for SQLite is a good cross-platform tool for exploring and querying the SQLite version of the BC Assessment data: https://sqlitebrowser.org The database consists of 13 tables. Table and field names correspond to the XML elements and CSV fields described in the BC Assessment user guides available from https://www1.bcassessment.ca/Support/Guide . Table name Documentation address Data Advice user guide assessmentAreaKey Data Advice user guide commercialInventory Commercial Inventory Extract user guide folio Data Advice user guide folioDescription Data Advice user guide jurisdictionKey Data Advice user guide legalDescription Data Advice user guide metadata not a data table; info about the database residentialInventory Residential Inventory Extract user guide sales Data Advice user guide valuation Data Advice user guide value Data Advice user guide Important All fields in the BC Assessment SQLite database are in the \"text\" data type by default. Joining tables \u00b6 Most users will need to join tables to access the data that interests them. The 10 tables derived from the Data Advice product can be joined on the folio_id field. The two tables derived from Inventory Extract products ( residentialInventory and commercialInventory ) do not have folio IDs but can be joined to the folio table using roll_number and jurisdiction . Important Roll numbers are unique within a jurisdiction but may be repeated across jurisdictions. When joining on roll_number also join on jurisdiction to avoid unintended results. Sales data \u00b6 The sales table contains data about the three most recent sales for each property in the database, regardless of when they occurred. The SQLite database for a given year includes most sales for that year but it does not necessarily provide a complete picture for previous periods. Assessed property values \u00b6 The valuation table contains assessed values for each folio, reported as landValue and improvementValue . The value table contains similar information but with additional values for BC Transit and School purposes (see the valueType field). For most questions about assessed value use the valuation table. Some elements in the source XML are repeatable, including valuation. This means that one folio can have more than one entry in the valuation table (each with a different taxExemptCOde or propertyClassCode , for example). Consult the documentation and ensure your analysis accounts for multiple entries. SQL queries \u00b6 With the SQLite browser it's easy to filter tables and export to CSV, but SQL queries are the most efficient way to extract a subset of the data that fits your parameters. If you are new to SQL the W3Schools SQL Tutorial is a good place to learn the basics. abacusBCA package \u00b6 The abacusBCA R package created by Jens von Bergmann facilitates data access and basic processing of BC Assessment data hosted on Abacus for research purposes. It has the ability to conveniently download the data and open a local sqlite connection. Users will have to specify their own abacus API token as environment variable to use this. Link to github repository: https://github.com/mountainMath/abacusBCA Link to documentation: https://mountainmath.github.io/abacusbca/","title":"BC Assessment data"},{"location":"dataset_code/bc_assessment/bc_assessment/#data-format-and-conversion","text":"BC Assessment provides data in XML and TXT format. To make the large XML files more accessible to researchers, UBC Library converts each to a series of CSV files and a SQLite database. Both the original files and the transformed versions are available in Abacus. The SQLite version is recommended for most users. For those interested in the file conversion, the process to create the SQLite database from the original XML and TXT files is outlined in these files: BCA_data_transformation_notes.txt describes the steps involved BCA_REVD_stylesheet.xsl is the XSLT stylesheet used in the transformation","title":"Data format and conversion"},{"location":"dataset_code/bc_assessment/bc_assessment/#using-the-sqlite-database","text":"The open source DB Browser for SQLite is a good cross-platform tool for exploring and querying the SQLite version of the BC Assessment data: https://sqlitebrowser.org The database consists of 13 tables. Table and field names correspond to the XML elements and CSV fields described in the BC Assessment user guides available from https://www1.bcassessment.ca/Support/Guide . Table name Documentation address Data Advice user guide assessmentAreaKey Data Advice user guide commercialInventory Commercial Inventory Extract user guide folio Data Advice user guide folioDescription Data Advice user guide jurisdictionKey Data Advice user guide legalDescription Data Advice user guide metadata not a data table; info about the database residentialInventory Residential Inventory Extract user guide sales Data Advice user guide valuation Data Advice user guide value Data Advice user guide Important All fields in the BC Assessment SQLite database are in the \"text\" data type by default.","title":"Using the sqlite database"},{"location":"dataset_code/bc_assessment/bc_assessment/#joining-tables","text":"Most users will need to join tables to access the data that interests them. The 10 tables derived from the Data Advice product can be joined on the folio_id field. The two tables derived from Inventory Extract products ( residentialInventory and commercialInventory ) do not have folio IDs but can be joined to the folio table using roll_number and jurisdiction . Important Roll numbers are unique within a jurisdiction but may be repeated across jurisdictions. When joining on roll_number also join on jurisdiction to avoid unintended results.","title":"Joining tables"},{"location":"dataset_code/bc_assessment/bc_assessment/#sales-data","text":"The sales table contains data about the three most recent sales for each property in the database, regardless of when they occurred. The SQLite database for a given year includes most sales for that year but it does not necessarily provide a complete picture for previous periods.","title":"Sales data"},{"location":"dataset_code/bc_assessment/bc_assessment/#assessed-property-values","text":"The valuation table contains assessed values for each folio, reported as landValue and improvementValue . The value table contains similar information but with additional values for BC Transit and School purposes (see the valueType field). For most questions about assessed value use the valuation table. Some elements in the source XML are repeatable, including valuation. This means that one folio can have more than one entry in the valuation table (each with a different taxExemptCOde or propertyClassCode , for example). Consult the documentation and ensure your analysis accounts for multiple entries.","title":"Assessed property values"},{"location":"dataset_code/bc_assessment/bc_assessment/#sql-queries","text":"With the SQLite browser it's easy to filter tables and export to CSV, but SQL queries are the most efficient way to extract a subset of the data that fits your parameters. If you are new to SQL the W3Schools SQL Tutorial is a good place to learn the basics.","title":"SQL queries"},{"location":"dataset_code/bc_assessment/bc_assessment/#abacusbca-package","text":"The abacusBCA R package created by Jens von Bergmann facilitates data access and basic processing of BC Assessment data hosted on Abacus for research purposes. It has the ability to conveniently download the data and open a local sqlite connection. Users will have to specify their own abacus API token as environment variable to use this. Link to github repository: https://github.com/mountainMath/abacusBCA Link to documentation: https://mountainmath.github.io/abacusbca/","title":"abacusBCA package"},{"location":"dataset_code/dataverse_utils/","text":"Dataverse utilities \u00b6 This is a Python library and generalized set of utilities which help with managing Dataverse repositories. These utilities are ad-hoc, written as needed and may be subject to change. Luckily they use git, so if they stop working for you it's possible to roll back to a version that does. Technology is magic. The defaults are mostly to work the University of British Columbia Library's installation of Dataverse, https://abacus.library.ubc.ca , but can of course be changed. For more details, please see the docs directory, beginning with index.md The primary repository for this software is at https://github.com/ubc-library-rc/dataverse_utils , and user-friendly documentation is available at https://ubc-library-rc.github.io/dataverse_utils .","title":"Dataverse utilities"},{"location":"dataset_code/dataverse_utils/#dataverse-utilities","text":"This is a Python library and generalized set of utilities which help with managing Dataverse repositories. These utilities are ad-hoc, written as needed and may be subject to change. Luckily they use git, so if they stop working for you it's possible to roll back to a version that does. Technology is magic. The defaults are mostly to work the University of British Columbia Library's installation of Dataverse, https://abacus.library.ubc.ca , but can of course be changed. For more details, please see the docs directory, beginning with index.md The primary repository for this software is at https://github.com/ubc-library-rc/dataverse_utils , and user-friendly documentation is available at https://ubc-library-rc.github.io/dataverse_utils .","title":"Dataverse utilities"},{"location":"dataset_code/dataverse_utils/LICENSE/","text":"The MIT License (MIT) Copyright \u00a9 2021 University of British Columbia Library Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWAR","title":"LICENSE"},{"location":"dataset_code/dataverse_utils/dataverse_utils/data/LDC_EULA_general/","text":"Linguistic Data Consortium Data Use Agreement \u00b6 A. Except as to the extent prohibited by any user agreement, the user shall have the right to \u00b6 incorporate portions of the LDC (Linguistic Data Consortium) data into its own work products for internal, non-commercial use and not for redistribution, incorporate small excerpts of text or audio data from the LDC data for display or publication in a scientific or technical context, but only for the purpose of descriving the research and related issues, and publish statistics and other summaries of the LDC data. B. License \u00b6 Except as otherwise provided herein, the user shall have no right to copy, redistribute, transmit, publish, sell, transfer, or otherwise use the LDC data for any purpose. The user shall give appropriate attribution to the LDC data in all scholarly or similar publications for which the LDC data or potions thereof have been used. C. Access to Individual Users \u00b6 Only individuals who are then-current faculty, students or staff members of LDC Member institutions or consultants or individuals providing services or doing research for Member institutions shall have access to the LDC data. D. Copyright \u00b6 The LDC data is protected by copyright as a collective work or compilation under the laws of the United States and other countries. All content, material, and other elements comprising LDC data are also copyrighted works. Users must abide by all additional copyright notices or restrictions contained in the LDC data license agreement supplements.","title":"LDC EULA general"},{"location":"dataset_code/dataverse_utils/dataverse_utils/data/LDC_EULA_general/#linguistic-data-consortium-data-use-agreement","text":"","title":"Linguistic Data Consortium Data Use Agreement"},{"location":"dataset_code/dataverse_utils/dataverse_utils/data/LDC_EULA_general/#a-except-as-to-the-extent-prohibited-by-any-user-agreement-the-user-shall-have-the-right-to","text":"incorporate portions of the LDC (Linguistic Data Consortium) data into its own work products for internal, non-commercial use and not for redistribution, incorporate small excerpts of text or audio data from the LDC data for display or publication in a scientific or technical context, but only for the purpose of descriving the research and related issues, and publish statistics and other summaries of the LDC data.","title":"A. Except as to the extent prohibited by any user agreement, the user shall have the right to"},{"location":"dataset_code/dataverse_utils/dataverse_utils/data/LDC_EULA_general/#b-license","text":"Except as otherwise provided herein, the user shall have no right to copy, redistribute, transmit, publish, sell, transfer, or otherwise use the LDC data for any purpose. The user shall give appropriate attribution to the LDC data in all scholarly or similar publications for which the LDC data or potions thereof have been used.","title":"B. License"},{"location":"dataset_code/dataverse_utils/dataverse_utils/data/LDC_EULA_general/#c-access-to-individual-users","text":"Only individuals who are then-current faculty, students or staff members of LDC Member institutions or consultants or individuals providing services or doing research for Member institutions shall have access to the LDC data.","title":"C. Access to Individual Users"},{"location":"dataset_code/dataverse_utils/dataverse_utils/data/LDC_EULA_general/#d-copyright","text":"The LDC data is protected by copyright as a collective work or compilation under the laws of the United States and other countries. All content, material, and other elements comprising LDC data are also copyrighted works. Users must abide by all additional copyright notices or restrictions contained in the LDC data license agreement supplements.","title":"D. Copyright"},{"location":"dataset_code/dataverse_utils/docs/","text":"Dataverse utilities \u00b6 This is a generalized set of utilities which help with managing Dataverse repositories. These utilities are ad-hoc , written as needed and subject to change. That being said, the effort is being made to make this a useful library. Source code (and this documentation) is available at the Github repository https://github.com/ubc-library-rc/dataverse_utils , and the user-friendly version of the documentation is at https://ubc-library-rc.github.io/dataverse_utils . Installation \u00b6 Any installation will require the use of the command line/command prompt. The easiest installation is with pip : pip install git + https : // github . com / ubc - library - rc / dataverse_utils @master Upgrading \u00b6 Upgrading is slightly different than the usual pip method, because it's not coming from pypi: pip install --upgrade git+https://github.com/ubc-library-rc/dataverse_utils@master Sadly, it's more to type than a normal pip upgrade string. Other methods of installing Python packages can be found at https://packaging.python.org/tutorials/installing-packages/ . If you have mkdocs installed, you can view the documentation in a web browser by running mkdocs from the top level directory of the Github repo by running mkdocs serve . The components \u00b6 Scripts \u00b6 There are six (6) scripts currently available. dv_del.py : Bulk (unpublished) file deletion utility dv_ldc_uploader.py : A utility which scrapes Linguistic Data Consortium metadata from their website, converts it to Dataverse JSON and uploads it, with the possibility of including local files. dv_manifest_gen.py : Creates a simple tab-separated value format file which can be edited and then used to upload files as well as file-level metadata. Normally files will be edited after creation, usually in a spreadsheet like Excel. dv_release.py : A bulk release utility. Either releases all the unreleased studies in a Dataverse or individually if persistent identifiers are available. dv_upload_tsv.py : Takes a tsv file in the format from dv_manifest_gen.py and does all the uploading and metadata entry. dv_pg_facet_date.py : A server-based tool which updates the publication date facet and performs a study reindex. More information about these can be found on the scripts page . Python library: dataverse_utils \u00b6 The default feature set from import dataverse_utils (or, more easily, import dataverse_utils as du is designed to work with data already present locally. The idea of this portion is to create a tsv file manifest for files which are to be uploaded to a Dataverse instance. Once the manifest is created, it's edited to add descriptive metadata, file paths and tags. Then, the manifest is used to upload those files to a an existing Dataverse study. import dataverse_utils as du du . dump_tsv ( '.' , '/Users/you/tmp/testme.tsv' ) [ Edit the . tsv at this stage here ] du . upload_from_tsv ( fil = '/Users/you/tmp/testme.tsv' , hdl = 'hdl:PERSIST/ID' , dv = 'https://dataverse.invalid' apikey = 'IAM-YOUR-DVERSE-APIKEY' ) The tsv should be edited to have a description. Tags should be separated by commas in the \"Tags\" column. If you are using relative paths, make sure that the script you are using is reading from the correct location. ldc \u00b6 The ldc component represents the Linguistic Data Consortium or LDC. The ldc module is designed to harvest LDC metadata from its catalogue, convert it to Dataverse JSON, then upload it to a Dataverse installation. Once the study has been created, the general dataverse_utils module can handle the file uploading. The ldc module requires the dryad2dataverse package. Because of this, it requires a tiny bit more effort, because LDC material doesn't have the required metadata. Here's snippet that shows how it works. import dataverse_utils.ldc as ldc ldc . ds . constants . DV_CONTACT_EMAIL = 'iamcontact@test.invalid' ldc . ds . constants . DV_CONTACT_NAME = 'Generic Support Email' KEY = 'IAM-YOUR-DVERSE-APIKEY' stud = 'LDC2021T02' #LDC study number a = ldc . Ldc ( stud ) a . fetch_record () #Data goes into the 'ldc' dataverse info = a . upload_metadata ( url = 'https://dataverse.invalid' , key = KEY , dv = 'ldc' ) hdl = info [ 'data' ][ 'persistentId' ] with open ( '/Users/you/tmp/testme.tsv' ) as fil : du . upload_from_tsv ( fil , hdl = hdl , dv = 'https://dataverse.invalid' , apikey = KEY ) Note that one method uses key and the other apikey . This is what is known as ad hoc . More information is available at the API reference . Samples \u00b6 The sample directory contains Python scripts which demonstrate the usage of the dataverse_utils library. They're not necessarily complete examples or optimized. Or even present, intially. You know, ad_hoc .","title":"Dataverse utilities"},{"location":"dataset_code/dataverse_utils/docs/#dataverse-utilities","text":"This is a generalized set of utilities which help with managing Dataverse repositories. These utilities are ad-hoc , written as needed and subject to change. That being said, the effort is being made to make this a useful library. Source code (and this documentation) is available at the Github repository https://github.com/ubc-library-rc/dataverse_utils , and the user-friendly version of the documentation is at https://ubc-library-rc.github.io/dataverse_utils .","title":"Dataverse utilities"},{"location":"dataset_code/dataverse_utils/docs/#installation","text":"Any installation will require the use of the command line/command prompt. The easiest installation is with pip : pip install git + https : // github . com / ubc - library - rc / dataverse_utils @master","title":"Installation"},{"location":"dataset_code/dataverse_utils/docs/#upgrading","text":"Upgrading is slightly different than the usual pip method, because it's not coming from pypi: pip install --upgrade git+https://github.com/ubc-library-rc/dataverse_utils@master Sadly, it's more to type than a normal pip upgrade string. Other methods of installing Python packages can be found at https://packaging.python.org/tutorials/installing-packages/ . If you have mkdocs installed, you can view the documentation in a web browser by running mkdocs from the top level directory of the Github repo by running mkdocs serve .","title":"Upgrading"},{"location":"dataset_code/dataverse_utils/docs/#the-components","text":"","title":"The components"},{"location":"dataset_code/dataverse_utils/docs/#scripts","text":"There are six (6) scripts currently available. dv_del.py : Bulk (unpublished) file deletion utility dv_ldc_uploader.py : A utility which scrapes Linguistic Data Consortium metadata from their website, converts it to Dataverse JSON and uploads it, with the possibility of including local files. dv_manifest_gen.py : Creates a simple tab-separated value format file which can be edited and then used to upload files as well as file-level metadata. Normally files will be edited after creation, usually in a spreadsheet like Excel. dv_release.py : A bulk release utility. Either releases all the unreleased studies in a Dataverse or individually if persistent identifiers are available. dv_upload_tsv.py : Takes a tsv file in the format from dv_manifest_gen.py and does all the uploading and metadata entry. dv_pg_facet_date.py : A server-based tool which updates the publication date facet and performs a study reindex. More information about these can be found on the scripts page .","title":"Scripts"},{"location":"dataset_code/dataverse_utils/docs/#python-library-dataverse_utils","text":"The default feature set from import dataverse_utils (or, more easily, import dataverse_utils as du is designed to work with data already present locally. The idea of this portion is to create a tsv file manifest for files which are to be uploaded to a Dataverse instance. Once the manifest is created, it's edited to add descriptive metadata, file paths and tags. Then, the manifest is used to upload those files to a an existing Dataverse study. import dataverse_utils as du du . dump_tsv ( '.' , '/Users/you/tmp/testme.tsv' ) [ Edit the . tsv at this stage here ] du . upload_from_tsv ( fil = '/Users/you/tmp/testme.tsv' , hdl = 'hdl:PERSIST/ID' , dv = 'https://dataverse.invalid' apikey = 'IAM-YOUR-DVERSE-APIKEY' ) The tsv should be edited to have a description. Tags should be separated by commas in the \"Tags\" column. If you are using relative paths, make sure that the script you are using is reading from the correct location.","title":"Python library: dataverse_utils"},{"location":"dataset_code/dataverse_utils/docs/#samples","text":"The sample directory contains Python scripts which demonstrate the usage of the dataverse_utils library. They're not necessarily complete examples or optimized. Or even present, intially. You know, ad_hoc .","title":"Samples"},{"location":"dataset_code/dataverse_utils/docs/api_ref/","text":"API Reference \u00b6 dataverse_utils \u00b6 Generalized dataverse utilities dataverse_utils.dataverse_utils \u00b6 A collection of Dataverse utilities for file and metadata manipulation DvGeneralUploadError Objects \u00b6 class DvGeneralUploadError ( Exception ) Raised on non-200 URL response Md5Error Objects \u00b6 class Md5Error ( Exception ) Raised on md5 mismatch make_tsv \u00b6 make_tsv ( start_dir , in_list = None , def_tag = 'Data' , inc_header = True ) -> str Recurses the tree for files and produces tsv output with with headers 'file', 'description', 'tags'. The 'description' is the filename without an extension. Returns tsv as string. Arguments : start_dir : str Path to start directory in_list : list Input file list. Defaults to recursive walk of current directory. def_tag : str Default Dataverse tag (eg, Data, Documentation, etc) Separate tags with a comma: eg. ('Data, 2016') inc_header : bool Include header row dump_tsv \u00b6 dump_tsv ( start_dir , filename , in_list = None , def_tag = 'Data' , inc_header = True ) Dumps output of make_tsv manifest to a file. Arguments : start_dir : str Path to start directory in_list : list List of files for which to create manifest entries. Will default to recursive directory crawl def_tag : str Default Dataverse tag (eg, Data, Documentation, etc) Separate tags with an easily splitable character: eg. ('Data, 2016') inc_header : bool Include header for tsv. file_path \u00b6 file_path ( fpath , trunc = '' ) -> str Create relative file path from full path string file_path('/tmp/Data/2011/excelfile.xlsx', '/tmp/') 'Data/2011' file_path('/tmp/Data/2011/excelfile.xlsx', '/tmp') 'Data/2011' Arguments : fpath : str File location (ie, complete path) trunc : str Leftmost portion of path to remove check_lock \u00b6 check_lock ( dv_url , study , apikey ) -> bool Checks study lock status; returns True if locked. Arguments : dvurl : str URL of Dataverse installation study - str Persistent ID of study apikey : str API key for user force_notab_unlock \u00b6 force_notab_unlock ( study , dv_url , fid , apikey , try_uningest = True ) -> int Forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. Returns 0 if unlocked, file id if locked (and then unlocked). Arguments : study : str Persistent indentifer of study dv_url : str URL to base Dataverse installation fid : str File ID for file object apikey : str API key for user try_uningest : bool Try to uningest the file that was locked. - Default - True uningest_file \u00b6 uningest_file ( dv_url , fid , apikey , study = 'n/a' ) Tries to uningest a file that has been ingested. Requires superuser API key. Arguments : dv_url : str URL to base Dataverse installation fid : int or str File ID of file to uningest apikey : str API key for superuser study : str Optional handle parameter for log messages upload_file \u00b6 upload_file ( fpath , hdl , ** kwargs ) Uploads file to Dataverse study and sets file metadata and tags. Arguments : fpath : str file location (ie, complete path) hdl : str Dataverse persistent ID for study (handle or DOI) kwargs : dict other parameters. Acceptable keywords and contents are: dv : str REQUIRED url to base Dataverse installation - eg - 'https://abacus.library.ubc.ca' apikey : str REQUIRED API key for user descr : str OPTIONAL file description md5 : str OPTIONAL md5sum for file checking tags : list OPTIONAL list of text file tags. Eg ['Data', 'June 2020'] dirlabel : str OPTIONAL Unix style relative pathname for Dataverse file path: eg: path/to/file/ nowait : bool OPTIONAL Force a file unlock and uningest instead of waiting for processing to finish trunc : str OPTIONAL Leftmost portion of path to remove rest : bool OPTIONAL Restrict file. Defaults to false unless True supplied restrict_file \u00b6 restrict_file ( ** kwargs ) Restrict file in Dataverse study. Arguments : kwargs : dict other parameters. Acceptable keywords and contents are: One of pid or fid is required pid : str file persistent ID fid : str file database ID dv : str REQUIRED url to base Dataverse installation - eg - 'https://abacus.library.ubc.ca' apikey : str REQUIRED API key for user rest : bool On True, restrict. Default True upload_from_tsv \u00b6 upload_from_tsv ( fil , hdl , ** kwargs ) Utility for bulk uploading. Assumes fil is formatted as tsv with headers 'file', 'description', 'tags'. 'tags' field will be split on commas. Arguments : fil : filelike object Open file object or io.IOStream() hdl : str Dataverse persistent ID for study (handle or DOI) trunc : str Leftmost portion of Dataverse study file path to remove. - eg - trunc ='/home/user/' if the tsv field is '/home/user/Data/ASCII' would set the path for that line of the tsv to 'Data/ASCII'. Defaults to None. kwargs : dict other parameters. Acceptable keywords and contents are: dv : str REQUIRED url to base Dataverse installation - eg - 'https://abacus.library.ubc.ca' apikey : str REQUIRED API key for user rest : bool On True, restrict access. Default False dataverse_utils.ldc \u00b6 Creates dataverse JSON from Linguistic Data Consortium website page. Ldc Objects \u00b6 class Ldc ( ds . Serializer ) An LDC item (eg, LDC2021T01) __init__ \u00b6 | __init__ ( ldc ) Returns a dict with keys created from an LDC catalogue web page. Arguments : ldc : str Linguistic Consortium Catalogue Number (eg. 'LDC2015T05'. This is what forms the last part of the LDC catalogue URL. ldcJson \u00b6 | @property | ldcJson () Returns a JSON based on the LDC web page scraping dryadJson \u00b6 | @property | dryadJson () LDC metadata in Dryad JSON format dvJson \u00b6 | @property | dvJson () LDC metadata in Dataverse JSON format embargo \u00b6 | @property | embargo () Boolean indicating embargo status fileJson \u00b6 | @property | fileJson ( timeout = 45 ) Returns False: No attached files possible at LDC files \u00b6 | @property | files () Returns None. No files possible fetch_record \u00b6 | fetch_record ( url = None , timeout = 45 ) Downloads record from LDC website make_ldc_json \u00b6 | make_ldc_json () Returns a dict with keys created from an LDC catalogue web page. name_parser \u00b6 | @staticmethod | name_parser ( name ) Returns lastName/firstName JSON snippet from name Arguments : name : str A name make_dryad_json \u00b6 | make_dryad_json ( ldc = None ) Creates a Dryad-style dict from an LDC dictionary Arguments : ldc : dict Dictionary containing LDC data. Defaults to self.ldcJson find_block_index \u00b6 | @staticmethod | find_block_index ( dvjson , key ) Finds the index number of an item in Dataverse's idiotic JSON list Arguments : dvjson : dict Dataverse JSON key : str key for which to find list index make_dv_json \u00b6 | make_dv_json ( ldc = None ) Returns complete Dataverse JSON Arguments : ldc : dict LDC dictionary. Defaults to self.ldcJson upload_metadata \u00b6 | upload_metadata ( ** kwargs ) -> dict Uploads metadata to dataverse Returns json from connection attempt. Arguments : kwargs: url : str base url to Dataverse key : str api key dv : str Dataverse to which it is being uploaded","title":"API reference"},{"location":"dataset_code/dataverse_utils/docs/api_ref/#api-reference","text":"","title":"API Reference"},{"location":"dataset_code/dataverse_utils/docs/api_ref/#dataverse_utils","text":"Generalized dataverse utilities","title":"dataverse_utils"},{"location":"dataset_code/dataverse_utils/docs/api_ref/#dataverse_utilsdataverse_utils","text":"A collection of Dataverse utilities for file and metadata manipulation","title":"dataverse_utils.dataverse_utils"},{"location":"dataset_code/dataverse_utils/docs/api_ref/#dvgeneraluploaderror-objects","text":"class DvGeneralUploadError ( Exception ) Raised on non-200 URL response","title":"DvGeneralUploadError Objects"},{"location":"dataset_code/dataverse_utils/docs/api_ref/#md5error-objects","text":"class Md5Error ( Exception ) Raised on md5 mismatch","title":"Md5Error Objects"},{"location":"dataset_code/dataverse_utils/docs/api_ref/#dataverse_utilsldc","text":"Creates dataverse JSON from Linguistic Data Consortium website page.","title":"dataverse_utils.ldc"},{"location":"dataset_code/dataverse_utils/docs/api_ref/#ldc-objects","text":"class Ldc ( ds . Serializer ) An LDC item (eg, LDC2021T01)","title":"Ldc Objects"},{"location":"dataset_code/dataverse_utils/docs/credits/","text":"Credits \u00b6 Dataverse_utils and their associated command line programs were written by Paul Lesack . Testing and UI suggestions from Jeremy Buhler . This is a product of the University of British Columbia Library Research Commons .","title":"Credits"},{"location":"dataset_code/dataverse_utils/docs/credits/#credits","text":"Dataverse_utils and their associated command line programs were written by Paul Lesack . Testing and UI suggestions from Jeremy Buhler . This is a product of the University of British Columbia Library Research Commons .","title":"Credits"},{"location":"dataset_code/dataverse_utils/docs/faq/","text":"Frequently asked questions \u00b6 \"Frequently\" may be relative. 1. I'm using Windows and the scripts don't seem to be working/recognized by [choice of command line interface here] \u00b6 There are (at least) four different ways to get some sort of command line access. The traditional command line, PowerShell, via SSH and Git bash. That's not even including the linux subsystem. The document on Windows script troubles gives common solutions. 2. I am using Windows [7-10]. I've installed via pip using a virtual environment, but they don't use my virtual environment's Python. \u00b6 What seems like a simple problem is surprisingly complex, as outlined here: https://matthew-brett.github.io/pydagogue/installing_scripts.html . This is further complicated by some other factors. If you have only one Python installation on your machine, you are probably OK. Mind you, if you're looking at this and that's the case, maybe I'm wrong. If you have multiple Pythons, you can try changing your environment variables to point to the correct Python. If you are using ArcGIS products, ArcGIS may write items into your Windows registry which will associate any .py file with it's grossly outdated Python 2.7, or just the wrong Python. More specifically: Computer\\HKEY_CLASSES_ROOT\\Python.File\\shell\\open\\command in the Windows registry. Unfortunately, the Windows computer that I have available for testing (as I normally use Linux or Mac) does not have an administrator account, so I can't confirm that changing this key will work (although there's no reason to believe it won't). This is a pain. Is there something less irritating that would work? \u00b6 Yes, there is. You can still run the scripts manually. There are two options for this. Download the repository via git to a convenient place and use the files in the scripts/ directory Point your %PATH% to where the scripts are installed. To find out where they are installed: TLDR: version: Point your %PATH% (and use first part of the path) to point to your [ venv ] \\ Scripts directory, because they're probably there. Long winded instructions/explanation: Start a Python session: import sys sys.path [x for x in sys.path if x.endswith('site-packages')] The location of the scripts will be written in [ whatever the output of sys.path ]/ dataverse_utils [ somestuff ] egg - info / installed - files . txt , usually three levels up in the scripts directory. Ironically, this is also the location of the activate portion of the comand required to start a virtual environment (if you are using a virtual environment). For some background on this, venv in Linux and Mac versions of Python uses bin , and reserves scripts for the scripts directory. Windows, however, uses Scripts for the venv module, and to make it worse it's not usually case sensitive, so anything in scripts gets put into Scripts .","title":"FAQ"},{"location":"dataset_code/dataverse_utils/docs/faq/#frequently-asked-questions","text":"\"Frequently\" may be relative.","title":"Frequently asked questions"},{"location":"dataset_code/dataverse_utils/docs/faq/#1-im-using-windows-and-the-scripts-dont-seem-to-be-workingrecognized-by-choice-of-command-line-interface-here","text":"There are (at least) four different ways to get some sort of command line access. The traditional command line, PowerShell, via SSH and Git bash. That's not even including the linux subsystem. The document on Windows script troubles gives common solutions.","title":"1. I'm using Windows and the scripts don't seem to be working/recognized by [choice of command line interface here]"},{"location":"dataset_code/dataverse_utils/docs/faq/#2-i-am-using-windows-7-10-ive-installed-via-pip-using-a-virtual-environment-but-they-dont-use-my-virtual-environments-python","text":"What seems like a simple problem is surprisingly complex, as outlined here: https://matthew-brett.github.io/pydagogue/installing_scripts.html . This is further complicated by some other factors. If you have only one Python installation on your machine, you are probably OK. Mind you, if you're looking at this and that's the case, maybe I'm wrong. If you have multiple Pythons, you can try changing your environment variables to point to the correct Python. If you are using ArcGIS products, ArcGIS may write items into your Windows registry which will associate any .py file with it's grossly outdated Python 2.7, or just the wrong Python. More specifically: Computer\\HKEY_CLASSES_ROOT\\Python.File\\shell\\open\\command in the Windows registry. Unfortunately, the Windows computer that I have available for testing (as I normally use Linux or Mac) does not have an administrator account, so I can't confirm that changing this key will work (although there's no reason to believe it won't).","title":"2. I am using Windows [7-10]. I've installed via pip using a virtual environment, but they don't use my virtual environment's Python."},{"location":"dataset_code/dataverse_utils/docs/faq/#this-is-a-pain-is-there-something-less-irritating-that-would-work","text":"Yes, there is. You can still run the scripts manually. There are two options for this. Download the repository via git to a convenient place and use the files in the scripts/ directory Point your %PATH% to where the scripts are installed. To find out where they are installed: TLDR: version: Point your %PATH% (and use first part of the path) to point to your [ venv ] \\ Scripts directory, because they're probably there. Long winded instructions/explanation: Start a Python session: import sys sys.path [x for x in sys.path if x.endswith('site-packages')] The location of the scripts will be written in [ whatever the output of sys.path ]/ dataverse_utils [ somestuff ] egg - info / installed - files . txt , usually three levels up in the scripts directory. Ironically, this is also the location of the activate portion of the comand required to start a virtual environment (if you are using a virtual environment). For some background on this, venv in Linux and Mac versions of Python uses bin , and reserves scripts for the scripts directory. Windows, however, uses Scripts for the venv module, and to make it worse it's not usually case sensitive, so anything in scripts gets put into Scripts .","title":"This is a pain. Is there something less irritating that would work?"},{"location":"dataset_code/dataverse_utils/docs/scripts/","text":"Utility scripts \u00b6 code { white-space : pre-wrap !important; } These scripts are available at the command line/command prompt and don't require any Python knowledge except how to install a Python library via pip, as outlined in the overview document. Once installed via pip, the scripts should be available via the command line and will not require calling Python explicitly. That is, they can be called from the command line directly. For example: dv_tsv_manifest.py is all you will need to type. Note that these programs have been primarily tested on Linux and MacOS, with Windows a tertiary priority . Windows is notable for its unusual file handling, so, as the MIT licenses stipulates, there is no warranty as to the suitability for a particular purpose. Of course, they should work. In no particular order: dv_del.py \u00b6 This is bulk deletion utility for unpublished studies (or even single studies). It's useful when your automated procedures have gone wrong, or if you don't feel like navigating through many menus. Note the -i switch which can ask for manual confirmation of deletions. Usage usage : dv_del . py [- h ] - k KEY [- d DATAVERSE | - p PID ] [- i ] [- u DVURL ] [-- version ] Delete draft studies from a Dataverse collection optional arguments : - h , -- help show this help message and exit - k KEY , -- key KEY Dataverse user API key - d DATAVERSE , -- dataverse DATAVERSE Dataverse collection short name from which to delete all draft records . eg . \"ldc\" - p PID , -- persistentId PID Handle or DOI to delete in format hdl : 11272.1 /FK2/ 12345 - i , -- interactive Confirm each study deletion - u DVURL , -- url DVURL URL to base Dataverse installation -- version Show version number and exit dv_manifest_gen.py \u00b6 Not technically a Dataverse-specific script, this utility will generate a tab-separated value output. The file consists of 3 columns: file, description and tags . Editing the result and using the upload utility to parse the tsv will add descriptive metadata, tags and file paths to an upload instead of laboriously using the Dataverse GUI. Tags may be separated by commas, eg: \"Data, SAS, June 2021\". Using stdout and a redirect will also save time. First dump a file as normal. Add other files to the end with different information using the exclude header switch -x and different tags along with output redirection >> . Usage usage : dv_manifest_gen . py [ - h ] [ - f FILENAME ] [ - t TAG ] [ - x ] [ - r ] [ - a ] [ -- version ] [ files [ files ... ]] Creates a file manifest in tab separated value format which can then be edited and used for file uploads to a Dataverse collection . Files can be edited to add file descriptions and comma - separated tags that will be automatically attached to metadata using products using the dataverse_utils library . Will dump to stdout unless - f or -- filename is used . Using the command and a dash ( ie , \"dv_manifest_gen.py -\" produces full paths for some reason . positional arguments : files Files to add to manifest . Leaving it blank will add all files in the current directory . If using - r will recursively show all . optional arguments : - h , -- help show this help message and exit - f FILENAME , -- filename FILENAME Save to file instead of outputting to stdout - t TAG , -- tag TAG Default tag ( s ) . Separate with comma and use quotes if there are spaces . eg . \"Data, June 2021\" . Defaults to \"Data\" - x , -- no - header Don 't include header in output. Useful if creating a complex tsv using redirects (ie, \">>\"). - r , -- recursive Recursive listing . - a , -- show - hidden Include hidden files . -- version Show version number and exit dv_upload_tsv.py \u00b6 Now that you have a tsv full of nicely described data, you can easily upload it to an existing study if you know the persistent ID and have an API key. For the best metadata, you should probably edit it manually to add correct descriptive metadata, notably the \"Description\" and \"Tags\". Tags are split separated by commas, so it's possible to have multiple tags for each data item, like \"Data, SPSS, June 2021\". File paths are automatically generated from the \"file\" column. Because of this, you should probably use relative paths rather than absolute paths unless you want to have a lengthy path string in Dataverse. Usage usage : dv_upload_tsv . py [ -h ] - p PID - k KEY [ -u URL ] [ -r ] [ -t TRUNCATE ] [ --version ] [ tsv ] Uploads data sets to an * existing * Dataverse study from the contents of a TSV ( tab separated value ) file . Metadata , file tags , paths , etc are all read from the TSV . JSON output from the Dataverse API is printed to stdout during the process . positional arguments : tsv TSV file to upload optional arguments : - h , -- help show this help message and exit - p PID , -- pid PID Dataverse study persistent identifier ( DOI / handle ) - k KEY , -- key KEY API key - u URL , -- url URL Dataverse installation base url . defaults to \"https://abacus.library.ubc.ca\" - r , -- restrict Restrict files after upload . - t TRUNCATE , -- truncate TRUNCATE Left truncate file path . As Dataverse studies can retain directory structure , you can set an arbitrary starting point by removing the leftmost portion . Eg : if the TSV has a file path of / home / user / Data / file . txt , setting -- truncate to \"/home/user\" would have file . txt in the Data directory in the Dataverse study . The file is still loaded from the path in the spreadsheet . Defaults to no truncation . --version Show version number and exit dv_release.py \u00b6 A bulk release utility for Dataverse. This utility will normally be used after a migration or large data transfer, such as a dryad2dataverse transfer from the Dryad data repository. It can release studies individually by persistent ID or just release all unreleased files in a Dataverse. Usage usage : dv_release . py [- h ] [- u URL ] - k KEY [- i ] [-- time STIME ] [- v ] [- r ] [- d DV | - p PID [ PID ...]] [-- version ] Bulk file releaser for unpublished Dataverse studies . Either releases individual studies or all unreleased studies in a single Dataverse collection . optional arguments : - h , -- help show this help message and exit - u URL , -- url URL Dataverse installation base URL . Default : https :// abacus . library . ubc . ca - k KEY , -- key KEY API key - i , -- interactive Manually confirm each release -- time STIME , - t STIME Time between release attempts in seconds . Default 10 - v Verbose mode - r , -- dry - run Only output a list of studies to be released - d DV , -- dv DV Short name of Dataverse collection to process ( eg : statcan ) - p PID [ PID ...], -- pid PID [ PID ...] Handles or DOIs to release in format hdl : 11272.1 /FK2/12345 or doi:10.80240/FK2/ NWRABI . Multiple values OK -- version Show version number and exit dv_ldc_uploader.py \u00b6 This is a very specialized utility which will scrape metadata from the Linguistic Data Consortium (LDC) and create a metadata record in a Dataverse. The LDC does not have an API, so the metadata is scraped from their web site. This means that the metadata may not be quite as controlled as that which comes from an API. Data from the LDC website is converted to Dryad -style JSON via dataverse_utils.ldc via the use of the dryad2dataverse library. There are two main methods of use for this utility: Multiple metadata uploads. Multiple LDC record numbers can be supplied and a study without files will be created for each one. If a TSV file with file information is upplied via the -t or --tsv switch, the utility will upload a single LDC study and upload the contents of the tsv file to the created record. Usage usage : dv_ldc_uploader . py [ - h ] [ - u URL ] - k KEY [ - d DVS ] [ - t TSV ] [ - n CNAME ] [ - e EMAIL ] [ - v ] [ -- version ] studies [ studies ... ] Linguistic Data Consortium metadata uploader for Dataverse . This utility will scrape the metadata from the LDC website ( https : // catalog . ldc . upenn . edu ) and upload data based on a TSV manifest . Please note that this utility was built with the Abacus repository ( https : // abacus . library . ubc . ca ) in mind , so many of the defaults are specific to that Dataverse installation . positional arguments : studies LDC Catalogue numbers to process , separated by spaces . eg . \"LDC2012T19 LDC2011T07\" . Case is ignored , so \"ldc2012T19\" will also work . optional arguments : - h , -- help show this help message and exit - u URL , -- url URL Dataverse installation base URL . Defaults to \"https://abacus.library.ubc.ca\" - k KEY , -- key KEY API key - d DVS , -- dvs DVS Short name of target Dataverse collection ( eg : ldc ) . Defaults to \"ldc\" - t TSV , -- tsv TSV Manifest tsv file for uploading and metadata . If not supplied , only metadata will be uploaded . Using this option requires only one positional * studies * argument - n CNAME , -- cname CNAME Study contact name . Default : \"Abacus support\" - e EMAIL , -- email EMAIL Dataverse study contact email address . Default : abacus - support @ lists . ubc . ca - v , -- verbose Verbose output -- version Show version number and exit dv_pg_facet_date.py \u00b6 This specialized tool is designed to be run on the server on which the Dataverse installation exists. When material is published in a Dataverse installation, the \"Publication Year\" facet in the Dataverse GUI is automatically populated with a date, which is the publication date in that Dataverse installation . This makes sense from the point of view of research data which is first deposited into a Dataverse installation, but fails as a finding aid for either; older data sets that have been migrated and reingested licensed data sets which may have been published years before they were purchased and ingested. For example, if you have a dataset that was published in 1971 but you only added it to your Dataverse installation in 2021, it is not necessarily intuitive to the end user that the \"publication date\" in this instance would be 2021. Ideally, you might like it to be 1971. Unfortunately, there is no API-based tool to manage this date. The only way to change it, as of late 2021, is to modify the underlying PostgreSQL database directly with the desired date. Subsequently, the study must be reindexed so that the revised publication date appears as an option in the facet. This tool will perform those operations. However, the tool must be run on the server on which the Dataverse installation exists, as reindexing API calls must be from localhost and database access is necessarily restricted. There are a few other prerequisites for using this tool which differ from the rest of the scripts included in this package. The user must have shell access to the server hosting the Dataverse installation Python 3.6 or higher must be installed The user must possess a valid Dataverse API key The user must know the PostgreSQL password If the database name and user have been changed, the user must know this as well The script requires the manual installation of psycopg2-binary or have a successfully compiled psycopg2 package for Python. See https://www.psycopg.org/docs/ . This is not installed with the normal pip install of the dataverse_utils package as none of the other scripts require it and, in general, the odds of someone using this utility are low. If you forget to install it, the program will politely remind you. This cannot be stressed enough. This tool will directly change values within the PostgreSQL database which holds all of Dataverse's information . Use this at your own risk; no warranty is implied and no responsibility will be accepted for data loss, etc. If any of the items listed make no sense to you or sound like gibberish, do not use this tool. Because editing the underlying database may have a high pucker factor for some, there is both a dry-run option and an option to just dump out SQL instead of actually touching anything. These two options do not perform a study reindex and don't alter the contents of the database. Usage usage : dv_pg_facet_date . py [- h ] [- d DBNAME ] [- u USER ] - p PASSWORD [- r | - o ] [- s ] - k KEY [- w URL ] [-- version ] pids [ pids ...] { distributionDate , productionDate , dateOfDeposit , dist , prod , dep } A utility to change the 'Production Date' web interface facet in a Dataverse installation to one of the three acceptable date types : 'distributionDate' , 'productionDate' , or 'dateOfDeposit' . This must be done in the PostgreSQL database directly , so this utility must be run on the * server * that hosts a Dataverse installation . Back up your database if you are unsure . positional arguments : pids persistentIdentifier { distributionDate , productionDate , dateOfDeposit , dist , prod , dep } date type which is to be shown in the facet . The short forms are aliases for the long forms . optional arguments : - h , -- help show this help message and exit - d DBNAME , -- dbname DBNAME Database name - u USER , -- user USER PostgreSQL username - p PASSWORD , -- password PASSWORD PostgreSQL password - r , -- dry - run print proposed SQL to stdout - o , -- sql - only dump sql to file called * pg_sql . sql * in current directory . Appends to file if it exists - s , -- save - old Dump old values to tsv called * pg_changed . tsv * in current directory . Appends to file if it exists - k KEY , -- key KEY API key for Dataverse installation . - w URL , -- url URL URL for base Dataverse installation . Default https :// abacus . library . ubc . ca -- version Show version number and exit THIS WILL EDIT YOUR POSTGRESQL DATABASE DIRECTLY . USE AT YOUR OWN RISK . Notes for Windows users \u00b6 Command line scripts for Python may not necessarily behave the way they do in Linux/Mac, depending on how you access them. For detailed information on Windows systems, please see the Windows testing document","title":"Scripts"},{"location":"dataset_code/dataverse_utils/docs/scripts/#utility-scripts","text":"code { white-space : pre-wrap !important; } These scripts are available at the command line/command prompt and don't require any Python knowledge except how to install a Python library via pip, as outlined in the overview document. Once installed via pip, the scripts should be available via the command line and will not require calling Python explicitly. That is, they can be called from the command line directly. For example: dv_tsv_manifest.py is all you will need to type. Note that these programs have been primarily tested on Linux and MacOS, with Windows a tertiary priority . Windows is notable for its unusual file handling, so, as the MIT licenses stipulates, there is no warranty as to the suitability for a particular purpose. Of course, they should work. In no particular order:","title":"Utility scripts"},{"location":"dataset_code/dataverse_utils/docs/scripts/#dv_delpy","text":"This is bulk deletion utility for unpublished studies (or even single studies). It's useful when your automated procedures have gone wrong, or if you don't feel like navigating through many menus. Note the -i switch which can ask for manual confirmation of deletions. Usage usage : dv_del . py [- h ] - k KEY [- d DATAVERSE | - p PID ] [- i ] [- u DVURL ] [-- version ] Delete draft studies from a Dataverse collection optional arguments : - h , -- help show this help message and exit - k KEY , -- key KEY Dataverse user API key - d DATAVERSE , -- dataverse DATAVERSE Dataverse collection short name from which to delete all draft records . eg . \"ldc\" - p PID , -- persistentId PID Handle or DOI to delete in format hdl : 11272.1 /FK2/ 12345 - i , -- interactive Confirm each study deletion - u DVURL , -- url DVURL URL to base Dataverse installation -- version Show version number and exit","title":"dv_del.py"},{"location":"dataset_code/dataverse_utils/docs/scripts/#dv_manifest_genpy","text":"Not technically a Dataverse-specific script, this utility will generate a tab-separated value output. The file consists of 3 columns: file, description and tags . Editing the result and using the upload utility to parse the tsv will add descriptive metadata, tags and file paths to an upload instead of laboriously using the Dataverse GUI. Tags may be separated by commas, eg: \"Data, SAS, June 2021\". Using stdout and a redirect will also save time. First dump a file as normal. Add other files to the end with different information using the exclude header switch -x and different tags along with output redirection >> . Usage usage : dv_manifest_gen . py [ - h ] [ - f FILENAME ] [ - t TAG ] [ - x ] [ - r ] [ - a ] [ -- version ] [ files [ files ... ]] Creates a file manifest in tab separated value format which can then be edited and used for file uploads to a Dataverse collection . Files can be edited to add file descriptions and comma - separated tags that will be automatically attached to metadata using products using the dataverse_utils library . Will dump to stdout unless - f or -- filename is used . Using the command and a dash ( ie , \"dv_manifest_gen.py -\" produces full paths for some reason . positional arguments : files Files to add to manifest . Leaving it blank will add all files in the current directory . If using - r will recursively show all . optional arguments : - h , -- help show this help message and exit - f FILENAME , -- filename FILENAME Save to file instead of outputting to stdout - t TAG , -- tag TAG Default tag ( s ) . Separate with comma and use quotes if there are spaces . eg . \"Data, June 2021\" . Defaults to \"Data\" - x , -- no - header Don 't include header in output. Useful if creating a complex tsv using redirects (ie, \">>\"). - r , -- recursive Recursive listing . - a , -- show - hidden Include hidden files . -- version Show version number and exit","title":"dv_manifest_gen.py"},{"location":"dataset_code/dataverse_utils/docs/scripts/#dv_upload_tsvpy","text":"Now that you have a tsv full of nicely described data, you can easily upload it to an existing study if you know the persistent ID and have an API key. For the best metadata, you should probably edit it manually to add correct descriptive metadata, notably the \"Description\" and \"Tags\". Tags are split separated by commas, so it's possible to have multiple tags for each data item, like \"Data, SPSS, June 2021\". File paths are automatically generated from the \"file\" column. Because of this, you should probably use relative paths rather than absolute paths unless you want to have a lengthy path string in Dataverse. Usage usage : dv_upload_tsv . py [ -h ] - p PID - k KEY [ -u URL ] [ -r ] [ -t TRUNCATE ] [ --version ] [ tsv ] Uploads data sets to an * existing * Dataverse study from the contents of a TSV ( tab separated value ) file . Metadata , file tags , paths , etc are all read from the TSV . JSON output from the Dataverse API is printed to stdout during the process . positional arguments : tsv TSV file to upload optional arguments : - h , -- help show this help message and exit - p PID , -- pid PID Dataverse study persistent identifier ( DOI / handle ) - k KEY , -- key KEY API key - u URL , -- url URL Dataverse installation base url . defaults to \"https://abacus.library.ubc.ca\" - r , -- restrict Restrict files after upload . - t TRUNCATE , -- truncate TRUNCATE Left truncate file path . As Dataverse studies can retain directory structure , you can set an arbitrary starting point by removing the leftmost portion . Eg : if the TSV has a file path of / home / user / Data / file . txt , setting -- truncate to \"/home/user\" would have file . txt in the Data directory in the Dataverse study . The file is still loaded from the path in the spreadsheet . Defaults to no truncation . --version Show version number and exit","title":"dv_upload_tsv.py"},{"location":"dataset_code/dataverse_utils/docs/scripts/#dv_releasepy","text":"A bulk release utility for Dataverse. This utility will normally be used after a migration or large data transfer, such as a dryad2dataverse transfer from the Dryad data repository. It can release studies individually by persistent ID or just release all unreleased files in a Dataverse. Usage usage : dv_release . py [- h ] [- u URL ] - k KEY [- i ] [-- time STIME ] [- v ] [- r ] [- d DV | - p PID [ PID ...]] [-- version ] Bulk file releaser for unpublished Dataverse studies . Either releases individual studies or all unreleased studies in a single Dataverse collection . optional arguments : - h , -- help show this help message and exit - u URL , -- url URL Dataverse installation base URL . Default : https :// abacus . library . ubc . ca - k KEY , -- key KEY API key - i , -- interactive Manually confirm each release -- time STIME , - t STIME Time between release attempts in seconds . Default 10 - v Verbose mode - r , -- dry - run Only output a list of studies to be released - d DV , -- dv DV Short name of Dataverse collection to process ( eg : statcan ) - p PID [ PID ...], -- pid PID [ PID ...] Handles or DOIs to release in format hdl : 11272.1 /FK2/12345 or doi:10.80240/FK2/ NWRABI . Multiple values OK -- version Show version number and exit","title":"dv_release.py"},{"location":"dataset_code/dataverse_utils/docs/scripts/#dv_ldc_uploaderpy","text":"This is a very specialized utility which will scrape metadata from the Linguistic Data Consortium (LDC) and create a metadata record in a Dataverse. The LDC does not have an API, so the metadata is scraped from their web site. This means that the metadata may not be quite as controlled as that which comes from an API. Data from the LDC website is converted to Dryad -style JSON via dataverse_utils.ldc via the use of the dryad2dataverse library. There are two main methods of use for this utility: Multiple metadata uploads. Multiple LDC record numbers can be supplied and a study without files will be created for each one. If a TSV file with file information is upplied via the -t or --tsv switch, the utility will upload a single LDC study and upload the contents of the tsv file to the created record. Usage usage : dv_ldc_uploader . py [ - h ] [ - u URL ] - k KEY [ - d DVS ] [ - t TSV ] [ - n CNAME ] [ - e EMAIL ] [ - v ] [ -- version ] studies [ studies ... ] Linguistic Data Consortium metadata uploader for Dataverse . This utility will scrape the metadata from the LDC website ( https : // catalog . ldc . upenn . edu ) and upload data based on a TSV manifest . Please note that this utility was built with the Abacus repository ( https : // abacus . library . ubc . ca ) in mind , so many of the defaults are specific to that Dataverse installation . positional arguments : studies LDC Catalogue numbers to process , separated by spaces . eg . \"LDC2012T19 LDC2011T07\" . Case is ignored , so \"ldc2012T19\" will also work . optional arguments : - h , -- help show this help message and exit - u URL , -- url URL Dataverse installation base URL . Defaults to \"https://abacus.library.ubc.ca\" - k KEY , -- key KEY API key - d DVS , -- dvs DVS Short name of target Dataverse collection ( eg : ldc ) . Defaults to \"ldc\" - t TSV , -- tsv TSV Manifest tsv file for uploading and metadata . If not supplied , only metadata will be uploaded . Using this option requires only one positional * studies * argument - n CNAME , -- cname CNAME Study contact name . Default : \"Abacus support\" - e EMAIL , -- email EMAIL Dataverse study contact email address . Default : abacus - support @ lists . ubc . ca - v , -- verbose Verbose output -- version Show version number and exit","title":"dv_ldc_uploader.py"},{"location":"dataset_code/dataverse_utils/docs/scripts/#dv_pg_facet_datepy","text":"This specialized tool is designed to be run on the server on which the Dataverse installation exists. When material is published in a Dataverse installation, the \"Publication Year\" facet in the Dataverse GUI is automatically populated with a date, which is the publication date in that Dataverse installation . This makes sense from the point of view of research data which is first deposited into a Dataverse installation, but fails as a finding aid for either; older data sets that have been migrated and reingested licensed data sets which may have been published years before they were purchased and ingested. For example, if you have a dataset that was published in 1971 but you only added it to your Dataverse installation in 2021, it is not necessarily intuitive to the end user that the \"publication date\" in this instance would be 2021. Ideally, you might like it to be 1971. Unfortunately, there is no API-based tool to manage this date. The only way to change it, as of late 2021, is to modify the underlying PostgreSQL database directly with the desired date. Subsequently, the study must be reindexed so that the revised publication date appears as an option in the facet. This tool will perform those operations. However, the tool must be run on the server on which the Dataverse installation exists, as reindexing API calls must be from localhost and database access is necessarily restricted. There are a few other prerequisites for using this tool which differ from the rest of the scripts included in this package. The user must have shell access to the server hosting the Dataverse installation Python 3.6 or higher must be installed The user must possess a valid Dataverse API key The user must know the PostgreSQL password If the database name and user have been changed, the user must know this as well The script requires the manual installation of psycopg2-binary or have a successfully compiled psycopg2 package for Python. See https://www.psycopg.org/docs/ . This is not installed with the normal pip install of the dataverse_utils package as none of the other scripts require it and, in general, the odds of someone using this utility are low. If you forget to install it, the program will politely remind you. This cannot be stressed enough. This tool will directly change values within the PostgreSQL database which holds all of Dataverse's information . Use this at your own risk; no warranty is implied and no responsibility will be accepted for data loss, etc. If any of the items listed make no sense to you or sound like gibberish, do not use this tool. Because editing the underlying database may have a high pucker factor for some, there is both a dry-run option and an option to just dump out SQL instead of actually touching anything. These two options do not perform a study reindex and don't alter the contents of the database. Usage usage : dv_pg_facet_date . py [- h ] [- d DBNAME ] [- u USER ] - p PASSWORD [- r | - o ] [- s ] - k KEY [- w URL ] [-- version ] pids [ pids ...] { distributionDate , productionDate , dateOfDeposit , dist , prod , dep } A utility to change the 'Production Date' web interface facet in a Dataverse installation to one of the three acceptable date types : 'distributionDate' , 'productionDate' , or 'dateOfDeposit' . This must be done in the PostgreSQL database directly , so this utility must be run on the * server * that hosts a Dataverse installation . Back up your database if you are unsure . positional arguments : pids persistentIdentifier { distributionDate , productionDate , dateOfDeposit , dist , prod , dep } date type which is to be shown in the facet . The short forms are aliases for the long forms . optional arguments : - h , -- help show this help message and exit - d DBNAME , -- dbname DBNAME Database name - u USER , -- user USER PostgreSQL username - p PASSWORD , -- password PASSWORD PostgreSQL password - r , -- dry - run print proposed SQL to stdout - o , -- sql - only dump sql to file called * pg_sql . sql * in current directory . Appends to file if it exists - s , -- save - old Dump old values to tsv called * pg_changed . tsv * in current directory . Appends to file if it exists - k KEY , -- key KEY API key for Dataverse installation . - w URL , -- url URL URL for base Dataverse installation . Default https :// abacus . library . ubc . ca -- version Show version number and exit THIS WILL EDIT YOUR POSTGRESQL DATABASE DIRECTLY . USE AT YOUR OWN RISK .","title":"dv_pg_facet_date.py"},{"location":"dataset_code/dataverse_utils/docs/scripts/#notes-for-windows-users","text":"Command line scripts for Python may not necessarily behave the way they do in Linux/Mac, depending on how you access them. For detailed information on Windows systems, please see the Windows testing document","title":"Notes for Windows users"},{"location":"dataset_code/dataverse_utils/docs/windows/","text":"Running the dataverse_utils scripts under Windows \u00b6 On Mac and Linux, running the scripts supplied with dataverse_utils is straightforward. They're available at the command line, which means that you can simply run them by (for example): $ dv_manifest_gen.py followed by switches and variables as normal. Doing this results in output to the terminal window. This is not necessarily the case in Windows . This test case uses a new installation of https://python.org Python, v.3.9.6, installed in Windows using the GUI, so it's as basic a Windows installation as you can get. In this instance, dataverse_utils were installed using pip from the PowerShell, ie pip install git+https://github.com/ubc-library-rc/dataverse_utils . However, pip should be pip regardless of which console you use to install. Here's a handy guide to show the results of how/if the command line scripts run. This example uses dv_manifest.gen.py , but appears to be generally applicable to any scripts configured with setuptools.setup() (ie, setuptools . setup ( scripts =[ whatever ] ). If this means nothing to you because you're not distributing your own Python packages,, just skip to the table below. Windows Terminal type \u00b6 PowerShell \u00b6 Note that on these tests the user is not an administrator . Administrator results, in all likelihood, will be completely different, ideally better. Problem Attempting to run the script results in a window popping up for a nanosecond and no output. Solution This may not occur if the PowerShell is run as an administrator. What is happening here is that the script is running, but it's showing up in a separate window. Output can be created as normal, if you use the correct switches. Unfortunately, you won't be able to see what they are, because the popup window disappears, which is not helpful. There are three potential fixes. If you can run PowerShell as an administrator, that may solve the problem. Edit the PATHEXT environment variable to include the .py extension. Note that if the user PATHEXT is edited, the system will ignore the system PATHEXT, meaning that things like .exe files won't run unless the full name is typed (eg. 'notepad.exe'). So, if editing the user PATHEXT, make sure to include the system PATHEXT and append ;.PY to the string. Edit the PATHEXT for PowerShell itself, rather than on a system wide level. Editing $PROFILE to include the .py extension should allow the Python script to run within PowerShell. For instructions, see https://docs.microsoft.com/en-ca/powershell/module/microsoft.powershell.core/about/about_profiles?view=powershell-7.1 . Create a profile as per How to create a profile Within that profile, $ env : PATHEXT += \";.py\" Depending on the nature of your Windows installation, this may be disabled by the security policy, in which case you can also try the method above. Command prompt \u00b6 This is the traditional Windows command prompt (ie, cmd.exe ). The scripts should just work after being installed with pip, as this installation is the default. For example, run with: C : \\ > dv_manifest_gen . py [ arguments ] Obviously it don't type the C:\\> part. SSH session \u00b6 If using the built-in Windows SSH server, scripts should simply run as per the command prompt above. Windows SSH sessions default to using the Windows command prompt, not bash. Bash sessions under SSH should function if Bash is configured as below. Git Bash \u00b6 Git Bash is part of the git suite available here: https://git-scm.com/downloads . There are a few notable wrinkles with for use with Python. During installation of Git After v2.32.0.2 (and possibly earlier), you will have the option during the installation to \"Enable experimental support for pseudo consoles\". Doing this will allow you run Python directly from the bash shell like you would normally, and the scripts should function as per the command prompt above. As a bonus, enabling this feature seems to fix errors with pipes which formerly resulted in the stdout is not a tty error when piping shell output (for instance, to grep ). If you have not checked this box, you will need to add an alias to your .bashrc and/or .bash_profile : alias python='winpty python' alias pip='winpty pip' Either that, or you will need to start Python with winpty python , which is annoying. Similarly winpty pip . Even if you have not enabled pseudo-console support and didn't complete use option 2, the scripts should still function normally though. Having scripts work but Python not work is not optimal and confusing, so a solution is there even though it technically isn't required. There are many options for Git Bash installation; testing has not covered all possible permutations of installation options.","title":"Running the dataverse_utils scripts under Windows"},{"location":"dataset_code/dataverse_utils/docs/windows/#running-the-dataverse_utils-scripts-under-windows","text":"On Mac and Linux, running the scripts supplied with dataverse_utils is straightforward. They're available at the command line, which means that you can simply run them by (for example): $ dv_manifest_gen.py followed by switches and variables as normal. Doing this results in output to the terminal window. This is not necessarily the case in Windows . This test case uses a new installation of https://python.org Python, v.3.9.6, installed in Windows using the GUI, so it's as basic a Windows installation as you can get. In this instance, dataverse_utils were installed using pip from the PowerShell, ie pip install git+https://github.com/ubc-library-rc/dataverse_utils . However, pip should be pip regardless of which console you use to install. Here's a handy guide to show the results of how/if the command line scripts run. This example uses dv_manifest.gen.py , but appears to be generally applicable to any scripts configured with setuptools.setup() (ie, setuptools . setup ( scripts =[ whatever ] ). If this means nothing to you because you're not distributing your own Python packages,, just skip to the table below.","title":"Running the dataverse_utils scripts under Windows"},{"location":"dataset_code/dataverse_utils/docs/windows/#windows-terminal-type","text":"","title":"Windows Terminal type"},{"location":"dataset_code/dataverse_utils/docs/windows/#powershell","text":"Note that on these tests the user is not an administrator . Administrator results, in all likelihood, will be completely different, ideally better. Problem Attempting to run the script results in a window popping up for a nanosecond and no output. Solution This may not occur if the PowerShell is run as an administrator. What is happening here is that the script is running, but it's showing up in a separate window. Output can be created as normal, if you use the correct switches. Unfortunately, you won't be able to see what they are, because the popup window disappears, which is not helpful. There are three potential fixes. If you can run PowerShell as an administrator, that may solve the problem. Edit the PATHEXT environment variable to include the .py extension. Note that if the user PATHEXT is edited, the system will ignore the system PATHEXT, meaning that things like .exe files won't run unless the full name is typed (eg. 'notepad.exe'). So, if editing the user PATHEXT, make sure to include the system PATHEXT and append ;.PY to the string. Edit the PATHEXT for PowerShell itself, rather than on a system wide level. Editing $PROFILE to include the .py extension should allow the Python script to run within PowerShell. For instructions, see https://docs.microsoft.com/en-ca/powershell/module/microsoft.powershell.core/about/about_profiles?view=powershell-7.1 . Create a profile as per How to create a profile Within that profile, $ env : PATHEXT += \";.py\" Depending on the nature of your Windows installation, this may be disabled by the security policy, in which case you can also try the method above.","title":"PowerShell"},{"location":"dataset_code/dataverse_utils/docs/windows/#command-prompt","text":"This is the traditional Windows command prompt (ie, cmd.exe ). The scripts should just work after being installed with pip, as this installation is the default. For example, run with: C : \\ > dv_manifest_gen . py [ arguments ] Obviously it don't type the C:\\> part.","title":"Command prompt"},{"location":"dataset_code/dataverse_utils/docs/windows/#ssh-session","text":"If using the built-in Windows SSH server, scripts should simply run as per the command prompt above. Windows SSH sessions default to using the Windows command prompt, not bash. Bash sessions under SSH should function if Bash is configured as below.","title":"SSH session"},{"location":"dataset_code/dataverse_utils/docs/windows/#git-bash","text":"Git Bash is part of the git suite available here: https://git-scm.com/downloads . There are a few notable wrinkles with for use with Python. During installation of Git After v2.32.0.2 (and possibly earlier), you will have the option during the installation to \"Enable experimental support for pseudo consoles\". Doing this will allow you run Python directly from the bash shell like you would normally, and the scripts should function as per the command prompt above. As a bonus, enabling this feature seems to fix errors with pipes which formerly resulted in the stdout is not a tty error when piping shell output (for instance, to grep ). If you have not checked this box, you will need to add an alias to your .bashrc and/or .bash_profile : alias python='winpty python' alias pip='winpty pip' Either that, or you will need to start Python with winpty python , which is annoying. Similarly winpty pip . Even if you have not enabled pseudo-console support and didn't complete use option 2, the scripts should still function normally though. Having scripts work but Python not work is not optimal and confusing, so a solution is there even though it technically isn't required. There are many options for Git Bash installation; testing has not covered all possible permutations of installation options.","title":"Git Bash"},{"location":"dataset_code/pccf/pccf/","text":"Dataset name Postal Code Conversion File and Postal Code Conversion File Plus Permanent URL There are multiple PCCF/PCCF+ records in Abacus; this Abacus search URL will return many of them Data access rules Restricted to researchers currently affiliated with SFU, UBC, UNBC, and UVic Postal Code Conversion Files allow researchers to map postal codes to Census geographies. Data with postal codes can be aggregated to standard geographies for reporting purposes or combined with other data available by Census geography (e.g. census tract, census subdivision). Note PCCF files do not contain any socioeconomic or demographic data themselves, they are just a key for aggregating or combining data files from other sources. Postal Code Conversion Files come in two flavours, the PCCF and PCCF+ . Postal Code boundaries do not perfectly match census boundaries: one postal code may span multiple census subdivisions, for example. The PCCF and PCCF+ differ in how they handle this kind of overlap. The PCCF file contains a Single Link Indicator (SLI) field to indicate the geographic area with the majority of dwellings assigned to a particular postal code. The PCCF+ is a SAS program using a population weighted random algorithm to increase accuracy where postal codes span over more than one geographic area from Queen's University Library's Guide to the PCCF/PCCF+ Which PCCF/PCCF+ file should I use? \u00b6 There are more than 50 PCCF/PCCF+ files in Abacus with different dates. This is because Postal Code and Census geographies change over time. Choose a PCCF/PCCF+ file with a Postal Code date as close as possible to the collection date of the data that contains the Postal Codes. Guides to the PCCF/PCCF+ \u00b6 Queens University Library maintains an excellent guide on PCCF and PCCF+ files, including step-by-step instructions for using the PCCF+ with SAS: https://guides.library.queensu.ca/PCCF/PCCF . PCCF helper scripts \u00b6 Recent PCCF files in Abacus include an SPSS syntax file to parse the data and assigns labels, making it easier to work with. Older PCCF records may only come with data files (e.g. this 2003 version ). When no syntax file is available researchers can use other tools to parse the data. The R script below provides an example that can be adapted to other files and contexts. pccf_prep_script.R # PCCF processing script, for June 2003 postal codes, 2001 Census # source files at https://hdl.handle.net/11272.1/AB2/QA758U # script assumes source files are in R working directory: # # - pccf59_JUN03_fccp59.txt # - fed02.dat # - cd02.dat # - sac03.dat # install readr package install . packages ( \"readr\" ) # load readr for use library ( readr ) # read pccf data file and parse into columns based on 'record layout' on p13 of PCCF reference guide # store output as 'formatted_pccf' formatted_pccf <- read_fwf ( \"pccf59_JUN03_fccp59.txt\" , fwf_widths ( c ( 6 , 3 , 8 , 2 , 9 , 11 , 1 , 2 , 4 , 3 , 70 , 3 , 3 , 3 , 1 , 7 , 2 , 4 , 5 , 4 , 1 , 1 , 1 , 30 , 1 , 1 , 8 , 8 ), c ( \"Postal Code\" , \"FSA\" , \"DAuid\" , \"Block\" , \"Lat\" , \"Long\" , \"SLI\" , \"PR\" , \"CDuid\" , \"CSD\" , \"CSDname\" , \"CSDtype\" , \"CCS\" , \"SAC\" , \"SACtype\" , \"CTname\" , \"ER\" , \"DPL\" , \"FED96uid\" , \"UARA\" , \"UARAtype\" , \"Rep_Point\" , \"PCtype\" , \"Comm_Name\" , \"DMT\" , \"H_DMT\" , \"Birth_Date\" , \"Ref_Date\" ))) # import and format label files FED96 <- read_fwf ( \"fed02.dat\" , fwf_widths ( c ( 5 , 100 ), c ( \"FED96uid\" , \"FED96name\" ))) SAC <- read_fwf ( \"sac02.dat\" , fwf_widths ( c ( 3 , 100 ), c ( \"SAC\" , \"SACname\" ))) CD <- read_fwf ( \"cd02.dat\" , fwf_widths ( c ( 4 , 100 ), c ( \"CDuid\" , \"CDname\" ))) # merge to add labels tmp_pccf_1 <- merge ( formatted_pccf , FED96 , by = \"FED96uid\" , all . formatted_pccf = TRUE , all . FED96 = FALSE ) tmp_pccf_2 <- merge ( tmp_pccf_1 , SAC , by = \"SAC\" , all . tmp_pccf_1 = TRUE , all . SAC = FALSE ) pccf_with_labels <- merge ( tmp_pccf_2 , CD , by = \"CDuid\" , all . tmp_pccf_2 = TRUE , all . SAC = FALSE ) ## save merged and formatted data as .csv write . csv ( pccf_with_labels , \"pccf_with_labels.csv\" )","title":"Postal Code Conversion Files"},{"location":"dataset_code/pccf/pccf/#which-pccfpccf-file-should-i-use","text":"There are more than 50 PCCF/PCCF+ files in Abacus with different dates. This is because Postal Code and Census geographies change over time. Choose a PCCF/PCCF+ file with a Postal Code date as close as possible to the collection date of the data that contains the Postal Codes.","title":"Which PCCF/PCCF+ file should I use?"},{"location":"dataset_code/pccf/pccf/#guides-to-the-pccfpccf","text":"Queens University Library maintains an excellent guide on PCCF and PCCF+ files, including step-by-step instructions for using the PCCF+ with SAS: https://guides.library.queensu.ca/PCCF/PCCF .","title":"Guides to the PCCF/PCCF+"},{"location":"dataset_code/pccf/pccf/#pccf-helper-scripts","text":"Recent PCCF files in Abacus include an SPSS syntax file to parse the data and assigns labels, making it easier to work with. Older PCCF records may only come with data files (e.g. this 2003 version ). When no syntax file is available researchers can use other tools to parse the data. The R script below provides an example that can be adapted to other files and contexts. pccf_prep_script.R # PCCF processing script, for June 2003 postal codes, 2001 Census # source files at https://hdl.handle.net/11272.1/AB2/QA758U # script assumes source files are in R working directory: # # - pccf59_JUN03_fccp59.txt # - fed02.dat # - cd02.dat # - sac03.dat # install readr package install . packages ( \"readr\" ) # load readr for use library ( readr ) # read pccf data file and parse into columns based on 'record layout' on p13 of PCCF reference guide # store output as 'formatted_pccf' formatted_pccf <- read_fwf ( \"pccf59_JUN03_fccp59.txt\" , fwf_widths ( c ( 6 , 3 , 8 , 2 , 9 , 11 , 1 , 2 , 4 , 3 , 70 , 3 , 3 , 3 , 1 , 7 , 2 , 4 , 5 , 4 , 1 , 1 , 1 , 30 , 1 , 1 , 8 , 8 ), c ( \"Postal Code\" , \"FSA\" , \"DAuid\" , \"Block\" , \"Lat\" , \"Long\" , \"SLI\" , \"PR\" , \"CDuid\" , \"CSD\" , \"CSDname\" , \"CSDtype\" , \"CCS\" , \"SAC\" , \"SACtype\" , \"CTname\" , \"ER\" , \"DPL\" , \"FED96uid\" , \"UARA\" , \"UARAtype\" , \"Rep_Point\" , \"PCtype\" , \"Comm_Name\" , \"DMT\" , \"H_DMT\" , \"Birth_Date\" , \"Ref_Date\" ))) # import and format label files FED96 <- read_fwf ( \"fed02.dat\" , fwf_widths ( c ( 5 , 100 ), c ( \"FED96uid\" , \"FED96name\" ))) SAC <- read_fwf ( \"sac02.dat\" , fwf_widths ( c ( 3 , 100 ), c ( \"SAC\" , \"SACname\" ))) CD <- read_fwf ( \"cd02.dat\" , fwf_widths ( c ( 4 , 100 ), c ( \"CDuid\" , \"CDname\" ))) # merge to add labels tmp_pccf_1 <- merge ( formatted_pccf , FED96 , by = \"FED96uid\" , all . formatted_pccf = TRUE , all . FED96 = FALSE ) tmp_pccf_2 <- merge ( tmp_pccf_1 , SAC , by = \"SAC\" , all . tmp_pccf_1 = TRUE , all . SAC = FALSE ) pccf_with_labels <- merge ( tmp_pccf_2 , CD , by = \"CDuid\" , all . tmp_pccf_2 = TRUE , all . SAC = FALSE ) ## save merged and formatted data as .csv write . csv ( pccf_with_labels , \"pccf_with_labels.csv\" )","title":"PCCF helper scripts"},{"location":"en/guides/user/","text":"User guide \u00b6 While Abacus is designed to be as easy to use as possible, searching for and using data is complex by its very nature. To help you start to navigate Abacus, here are some basic instructions on key activities and functions. If you already know the basics and wish to explore further, please see the search syntax page and the Advanced User's Guide . Login \u00b6 Your first login \u00b6 If you have never logged in to Abacus, when you log in for the first time from your institutional web page, you will be taken to the account creation page. Abacus will automatically create an account which uses information provided by your institution. Verify that your account details are correct and confirm them. Click on the Abacus logo at the top left of the page, and you will be able to use the Abacus service. Info You will only need to perform the above on your very first login. The primary Abacus web page is located at https://abacus.library.ubc.ca . Anyone may search Abacus and view the data descriptions but many data files can only be accessed by users affiliated with SFU, UBC, UNBC, or UVic. To work with these licensed files you will need to log in with your university credentials. If you have come directly to the Abacus home page, as opposed to navigating from your institution's library web site, follow these instructions to log in : SFU, UBC, and UNBC users UVic users Use the Log In link at the extreme right of the top menu bar. By default, you will be presented with a drop down list showing SFU, UBC, and UNBC. Select your institution and login using the credentials supplied by your university, such as the Campus-wide Login (CWL) used at the University of British Columbia. If the drop-down list of institutions is not visible and you are being prompted for username/password authentication, you will see Other options underneath the dialogue boxes, with a button labelled Your Institution . Clicking on this button will produce the drop-down list as described above. It is not necessary to select a username/password for Abacus , an account will be created for you the first time you login. Your institutional login ID is all that is required to access and download files to which your university has access. Login here using your university credentials. After login you will be redirected to the UVic collection in Abacus. Search \u00b6 This section introduces Abacus search options. For more information see the search syntax page. Default search \u00b6 Searching is possible from almost anywhere inside Abacus. From the main page at https://abacus.library.ubc.ca , the search feature will search the entire contents of Abacus, including material to which your institution may not have access. To easily limit your search to material to which your account has access, select your university from the icons at the top of the page. To limit your search to material that is available to the public without login, select the Abacus Open Data icon. Should your search produce too many results, it can be narrowed by selecting the facets on the left side of the page, such as \"Producer Name\". Facets may also be discarded once applied by clicking on them after they appear at the top of the search results. Selected search tips \u00b6 Boolean operators . By default, search terms are connected with the Boolean OR operator. Use AND between search terms if you would like results to contain all your terms: e.g. adult AND literacy AND education . Searching for phrases . Enclose search terms in double quotation marks to find records that contain the exact phrase: e.g. \"adult literacy\" . Be careful not to inadvertently exclude material of interest: for example, a search for \"adult literacy survey\" will return the International Adult Literacy Survey (IALS) , but not the Adult Literacy and Life Skills Survey . Wildcards . Abacus supports single- and multiple-character wildcards: ? to match one character. ?oom matches \"zoom\", \"room\", and \"boom\", but not \"broom\". * to match zero or more characters. revolution* matches \"revolution\", \"revolutionary\", and \"revolutionaries\". Info Wildcard operators function only on terms, not phrases. For more detail and additional tips see Search syntax . Advanced search \u00b6 The Advanced search will allow you to search by very specific individual fields and to search within the data itself for applicable datasets. Note that not all datasets have variable-level searching implemented, nor does every study in Abacus have every field filled, as some values are not applicable to some types of datasets. Local vs global search \u00b6 Abacus supports two distinct search scopes : \"global\" and \"local\". Using the search box on the left-hand side of the user interface will limit results to the dataverse currently selected (a \"local\" search), while using the search box through selecting the \"Search\" drop-down menu at the top of the user interface will execute a query on all dataverses in the Abacus Data Network (a \"global\" search, as shown below). Search syntax \u00b6 Boolean logical operators \u00b6 Boolean logical operators greatly improve the power and efficiency of queries. By default, search terms are connected with the Boolean OR operator. Thus, the search adult literacy survey will match material containing \"adult\", \"literacy\", or \"survey\", but not necessarily all three. For results that contain all search terms, use the AND operator: adult AND literacy AND survey . In addition to the AND and OR operators, other useful Boolean operators include NOT , + , and - . The NOT and - operators function similarly, requiring that the term following the operator not be present in search results, while the + operator requires that the term is present. For instance, literacy NOT child , or literacy -child , will return all results containing \"literacy\" without the term \"child\", while +\"adult literacy\" survey returns results with the phrase \"adult literacy\" that may or may not contain the term \"survey\". In addition to their word form, Boolean operators can also be called in symbol form: Word Symbol AND && OR || NOT ! Info To use Boolean operators in word form (e.g. AND , OR , NOT ), all letters must be uppercase. Additionally, when using the ! operator, unlike its word form NOT , do not include a space before the term it applies to. correct: education !primary incorrect: education ! primary . For added control over the Boolean logic of a search, users can group clauses to form sub-queries using parentheses. Searching for vaccine AND (\"corona virus\" OR \"COVID-19\") will return material that matches either \"vaccine\" and \"corona virus\" or \"vaccine\" and \"COVID-19\". Querying specific fields \u00b6 Users can also narrow their search within Abacus by querying specific fields . The syntax for this requires, first, specifiying the field to be searched (e.g. title ), followed by a : , and then the term to search for: title : financial . (NOTE: There is no space between the colon and the search term. ) Searching for multiple terms within a field requires specifying the field before each term. For example, the sytnax for searching for the terms \"labour\", \"force\", and \"survey\" within the \"title\" field is title : labour AND title : force AND title : survey . Using the query title : labour force survey will search for \"labour\" in the \"title\" field and \"force\" and \"survey\" in any other metadata field. To search for the phrase \"labour force survey\" in the \"title\" field, wrap the phrase in double quotation marks: title:\"labour force survey\" . Specifying a date range \u00b6 Searching by specifying a date range can also be an effective way to find data. Range syntax is denoted in Abacus as [ TO ] , where both a lower and upper bound are specified. Different types of brackets indicate whether these bounds are inclusive or exclusive. Square brackets [] represent inclusive lower and upper bounds, whereas curly brackets {} represent exclusive bounds. These can be combined in a single query: {1956-02-16 TO 1988-05-03] . Dates in Abacus are formatted as YYYY-MM-DD , where: YYYY is the year, MM is the month, DD is the day of the month, Truncation: Users do not need to specify complete dates and times to search using date ranges. For instance, the search pubdate:[1956 TO 1998-05] using the \"publication date\" field will produce relevant material from (and including) 1 January 1956 through 31 May 1998. Wildcard: You may use the * wildcard as the lower or upper bound. Entering [* TO 2001-09-11} would return all matched items from the earliest in the repository up to (but excluding) 11 September 2001, whereas entering [2001-09-11 TO *] would return all matched items from (and including) 11 September 2001 up to the most recent matched deposit. Download \u00b6 Once you've found a data set in the search, click on the title and you'll be taken to its page. By default, you are taken to a short display with an abstract and a listing of files. For each file, you may have several options, depending on its type. To quickly download the file, click on the Download button For statistical files which are available for online analysis, you can view their contents and download in a variety of formats by using the Explore button. You will also be able to view summary statistics for these data sets and create cross-tabulations, as well as search within the data set itself at the variable level. Linking and downloading individual files \u00b6 To see a preview (if available), permanent URL and other metadata for a particular file, click on the filename . The next page will provide citations for both the data set and the individual file , as well as a record of any changes to the file itself. The file may be downloaded by clicking the Download button. The metadata tab will may also provide a direct download link (if available), and display an md5 checksum should you wish to verify your download. If there is no direct download link, and programmatic access is required, the using the equivalent of the following curl commands will allow direct download. Publicly accessible data files \u00b6 As an example, the 2020 Labour Force Survey data: File page for the August 2020 ASCII data : https://dvn.library.ubc.ca/file.xhtml?persistentId=hdl:11272.1/AB2/GGXMM2/KMGN1A&version=2.0 To download this file: curl \"https://abacus.library.ubc.ca/api/access/datafile/:persistentId/?persistentId=hdl:11272.1/AB2/GGXMM2/KMGN1A\" -o micro2020-08.zip\" Licensed data \u00b6 To download licensed data, you will need an API token (see the Advanced User's Guide ) As an example, the CanMap Content Suite, v2019.3 : File page for the WatershedsRegion.zip file: https://dvn.library.ubc.ca/file.xhtml?persistentId=hdl:11272.1/AB2/PCTBFN/IBVPVN&version=1.0 To download this file: curl -H \"X-Dataverse-key: YOURAPITOKEN\" \"https://abacus.library.ubc.ca/api/access/datafile/:persistentId/?persistentId=hdl:11272.1/AB2/PCTBFN/IBVPVN\" -o WatershedsRegion.zip Where YOURAPITOKEN is your account's API token. Bulk downloads \u00b6 To download multiple files at once, select the checkbox at the left of the top row of the file table. This will automatically select all of the files in the study. Download the files using Download button on the top row of the table. For data sets with very large files, such as the geospatial datasets from DMTI Spatial Inc. or compilations of lidar or orthoimagery, downloads may be truncated due to size limits on zip files. Other download options are available, such as API based downloads (as per the Advanced User's Guide ). Metadata \u00b6 To view the metadata or detailed description of the material in Abacus, select the Metadata tab of the data set in which you are viewing. By default, citation metadata is visible, with other sections of metadata (such as Geospatial data) is only visible after clicking on the appropriate button. For many social science data sets, much of the metadata will be stored in the Social Science and Humanities Metadata section, such as sample information, etc. Study metadata can be exported in a variety of formats for use in other applications by selecting the format desired from the Export Metadata button found just above the citation metadata. Some formats can be used with citation managers, such as DataCite and Dublin Core, but import formats are dependent entirely on the citation manager software. Help/support \u00b6 For more detailed information beyond this brief guide, please see the Advanced User's Guide , which will give a detailed overview of the use of Dataverse, the software which powers Abacus. Support within Abacus itself is available as well. For email assistance, use the Support link in the interface menu bar. For support with specific data sets, or to report errors and technical issues, within each record you can use the Contact button just above and to the right of the dataset title. Note that in most cases, the Support and Contact buttons will be sent to the same people, ie Abacus Support .","title":"User guide"},{"location":"en/guides/user/#user-guide","text":"While Abacus is designed to be as easy to use as possible, searching for and using data is complex by its very nature. To help you start to navigate Abacus, here are some basic instructions on key activities and functions. If you already know the basics and wish to explore further, please see the search syntax page and the Advanced User's Guide .","title":"User guide"},{"location":"en/guides/user/#login","text":"","title":"Login"},{"location":"en/guides/user/#your-first-login","text":"If you have never logged in to Abacus, when you log in for the first time from your institutional web page, you will be taken to the account creation page. Abacus will automatically create an account which uses information provided by your institution. Verify that your account details are correct and confirm them. Click on the Abacus logo at the top left of the page, and you will be able to use the Abacus service. Info You will only need to perform the above on your very first login. The primary Abacus web page is located at https://abacus.library.ubc.ca . Anyone may search Abacus and view the data descriptions but many data files can only be accessed by users affiliated with SFU, UBC, UNBC, or UVic. To work with these licensed files you will need to log in with your university credentials. If you have come directly to the Abacus home page, as opposed to navigating from your institution's library web site, follow these instructions to log in : SFU, UBC, and UNBC users UVic users Use the Log In link at the extreme right of the top menu bar. By default, you will be presented with a drop down list showing SFU, UBC, and UNBC. Select your institution and login using the credentials supplied by your university, such as the Campus-wide Login (CWL) used at the University of British Columbia. If the drop-down list of institutions is not visible and you are being prompted for username/password authentication, you will see Other options underneath the dialogue boxes, with a button labelled Your Institution . Clicking on this button will produce the drop-down list as described above. It is not necessary to select a username/password for Abacus , an account will be created for you the first time you login. Your institutional login ID is all that is required to access and download files to which your university has access. Login here using your university credentials. After login you will be redirected to the UVic collection in Abacus.","title":"Your first login"},{"location":"en/guides/user/#search","text":"This section introduces Abacus search options. For more information see the search syntax page.","title":"Search"},{"location":"en/guides/user/#default-search","text":"Searching is possible from almost anywhere inside Abacus. From the main page at https://abacus.library.ubc.ca , the search feature will search the entire contents of Abacus, including material to which your institution may not have access. To easily limit your search to material to which your account has access, select your university from the icons at the top of the page. To limit your search to material that is available to the public without login, select the Abacus Open Data icon. Should your search produce too many results, it can be narrowed by selecting the facets on the left side of the page, such as \"Producer Name\". Facets may also be discarded once applied by clicking on them after they appear at the top of the search results.","title":"Default search"},{"location":"en/guides/user/#advanced-search","text":"The Advanced search will allow you to search by very specific individual fields and to search within the data itself for applicable datasets. Note that not all datasets have variable-level searching implemented, nor does every study in Abacus have every field filled, as some values are not applicable to some types of datasets.","title":"Advanced search"},{"location":"en/guides/user/#local-vs-global-search","text":"Abacus supports two distinct search scopes : \"global\" and \"local\". Using the search box on the left-hand side of the user interface will limit results to the dataverse currently selected (a \"local\" search), while using the search box through selecting the \"Search\" drop-down menu at the top of the user interface will execute a query on all dataverses in the Abacus Data Network (a \"global\" search, as shown below).","title":"Local vs global search"},{"location":"en/guides/user/#search-syntax","text":"","title":"Search syntax"},{"location":"en/guides/user/#boolean-logical-operators","text":"Boolean logical operators greatly improve the power and efficiency of queries. By default, search terms are connected with the Boolean OR operator. Thus, the search adult literacy survey will match material containing \"adult\", \"literacy\", or \"survey\", but not necessarily all three. For results that contain all search terms, use the AND operator: adult AND literacy AND survey . In addition to the AND and OR operators, other useful Boolean operators include NOT , + , and - . The NOT and - operators function similarly, requiring that the term following the operator not be present in search results, while the + operator requires that the term is present. For instance, literacy NOT child , or literacy -child , will return all results containing \"literacy\" without the term \"child\", while +\"adult literacy\" survey returns results with the phrase \"adult literacy\" that may or may not contain the term \"survey\". In addition to their word form, Boolean operators can also be called in symbol form: Word Symbol AND && OR || NOT ! Info To use Boolean operators in word form (e.g. AND , OR , NOT ), all letters must be uppercase. Additionally, when using the ! operator, unlike its word form NOT , do not include a space before the term it applies to. correct: education !primary incorrect: education ! primary . For added control over the Boolean logic of a search, users can group clauses to form sub-queries using parentheses. Searching for vaccine AND (\"corona virus\" OR \"COVID-19\") will return material that matches either \"vaccine\" and \"corona virus\" or \"vaccine\" and \"COVID-19\".","title":"Boolean logical operators"},{"location":"en/guides/user/#querying-specific-fields","text":"Users can also narrow their search within Abacus by querying specific fields . The syntax for this requires, first, specifiying the field to be searched (e.g. title ), followed by a : , and then the term to search for: title : financial . (NOTE: There is no space between the colon and the search term. ) Searching for multiple terms within a field requires specifying the field before each term. For example, the sytnax for searching for the terms \"labour\", \"force\", and \"survey\" within the \"title\" field is title : labour AND title : force AND title : survey . Using the query title : labour force survey will search for \"labour\" in the \"title\" field and \"force\" and \"survey\" in any other metadata field. To search for the phrase \"labour force survey\" in the \"title\" field, wrap the phrase in double quotation marks: title:\"labour force survey\" .","title":"Querying specific fields"},{"location":"en/guides/user/#specifying-a-date-range","text":"Searching by specifying a date range can also be an effective way to find data. Range syntax is denoted in Abacus as [ TO ] , where both a lower and upper bound are specified. Different types of brackets indicate whether these bounds are inclusive or exclusive. Square brackets [] represent inclusive lower and upper bounds, whereas curly brackets {} represent exclusive bounds. These can be combined in a single query: {1956-02-16 TO 1988-05-03] . Dates in Abacus are formatted as YYYY-MM-DD , where: YYYY is the year, MM is the month, DD is the day of the month, Truncation: Users do not need to specify complete dates and times to search using date ranges. For instance, the search pubdate:[1956 TO 1998-05] using the \"publication date\" field will produce relevant material from (and including) 1 January 1956 through 31 May 1998. Wildcard: You may use the * wildcard as the lower or upper bound. Entering [* TO 2001-09-11} would return all matched items from the earliest in the repository up to (but excluding) 11 September 2001, whereas entering [2001-09-11 TO *] would return all matched items from (and including) 11 September 2001 up to the most recent matched deposit.","title":"Specifying a date range"},{"location":"en/guides/user/#download","text":"Once you've found a data set in the search, click on the title and you'll be taken to its page. By default, you are taken to a short display with an abstract and a listing of files. For each file, you may have several options, depending on its type. To quickly download the file, click on the Download button For statistical files which are available for online analysis, you can view their contents and download in a variety of formats by using the Explore button. You will also be able to view summary statistics for these data sets and create cross-tabulations, as well as search within the data set itself at the variable level.","title":"Download"},{"location":"en/guides/user/#linking-and-downloading-individual-files","text":"To see a preview (if available), permanent URL and other metadata for a particular file, click on the filename . The next page will provide citations for both the data set and the individual file , as well as a record of any changes to the file itself. The file may be downloaded by clicking the Download button. The metadata tab will may also provide a direct download link (if available), and display an md5 checksum should you wish to verify your download. If there is no direct download link, and programmatic access is required, the using the equivalent of the following curl commands will allow direct download.","title":"Linking and downloading individual files"},{"location":"en/guides/user/#bulk-downloads","text":"To download multiple files at once, select the checkbox at the left of the top row of the file table. This will automatically select all of the files in the study. Download the files using Download button on the top row of the table. For data sets with very large files, such as the geospatial datasets from DMTI Spatial Inc. or compilations of lidar or orthoimagery, downloads may be truncated due to size limits on zip files. Other download options are available, such as API based downloads (as per the Advanced User's Guide ).","title":"Bulk downloads"},{"location":"en/guides/user/#metadata","text":"To view the metadata or detailed description of the material in Abacus, select the Metadata tab of the data set in which you are viewing. By default, citation metadata is visible, with other sections of metadata (such as Geospatial data) is only visible after clicking on the appropriate button. For many social science data sets, much of the metadata will be stored in the Social Science and Humanities Metadata section, such as sample information, etc. Study metadata can be exported in a variety of formats for use in other applications by selecting the format desired from the Export Metadata button found just above the citation metadata. Some formats can be used with citation managers, such as DataCite and Dublin Core, but import formats are dependent entirely on the citation manager software.","title":"Metadata"},{"location":"en/guides/user/#helpsupport","text":"For more detailed information beyond this brief guide, please see the Advanced User's Guide , which will give a detailed overview of the use of Dataverse, the software which powers Abacus. Support within Abacus itself is available as well. For email assistance, use the Support link in the interface menu bar. For support with specific data sets, or to report errors and technical issues, within each record you can use the Contact button just above and to the right of the dataset title. Note that in most cases, the Support and Contact buttons will be sent to the same people, ie Abacus Support .","title":"Help/support"},{"location":"en/guides/user/search/","text":"Search syntax tips \u00b6 Boolean logical operators Search scope Querying specific fields Specifying a date range Boolean logical operators \u00b6 Boolean logical operators greatly improve the power and efficiency of queries. By default, search terms are connected with the Boolean OR operator. Thus, the search adult literacy survey will match material containing \"adult\", \"literacy\", or \"survey\", but not necessarily all three. For results that contain all search terms, use the AND operator: adult AND literacy AND survey . In addition to the AND and OR operators, other useful Boolean operators include NOT , + , and - . The NOT and - operators function similarly, requiring that the term following the operator not be present in search results, while the + operator requires that the term is present. For instance, literacy NOT child , or literacy -child , will return all results containing \"literacy\" without the term \"child\", while +\"adult literacy\" survey returns results with the phrase \"adult literacy\" that may or may not contain the term \"survey\". In addition to their word form, Boolean operators can also be called in symbol form: Word Symbol AND && OR || NOT ! To use Boolean operators in word form (e.g. AND , OR , NOT ), all letters must be uppercase. Additionally, when using the ! operator, unlike its word form NOT , do not include a space before the term it applies to--correct: education !primary , incorrect: education ! primary . For added control over the Boolean logic of a search, users can group clauses to form sub-queries using parentheses. Searching for vaccine AND (\"corona virus\" OR \"COVID-19\") will return material that matches either \"vaccine\" and \"corona virus\" or \"vaccine\" and \"COVID-19\". Search scope \u00b6 Abacus supports two distinct search scopes : \"global\" and \"local\". Using the search box on the left-hand side of the user interface will limit results to the dataverse currently selected (a \"local\" search), while using the search box through selecting the \"Search\" drop-down menu at the top of the user interface will execute a query on all dataverses in the Abacus Data Network (a \"global\" search). Querying specific fields \u00b6 Users can also narrow their search within Abacus by querying specific fields . The syntax for this requires, first, specifiying the field to be searched (e.g. title ), followed by a : , and then the term to search for: title : financial . (NOTE: There is no space between the colon and the search term. ) Searching for multiple terms within a field requires specifying the field before each term. For example, the sytnax for searching for the terms \"labour\", \"force\", and \"survey\" within the \"title\" field is title : labour AND title : force AND title : survey . Using the query title : labour force survey will search for \"labour\" in the \"title\" field and \"force\" and \"survey\" in any other metadata field. To search for the phrase \"labour force survey\" in the \"title\" field, wrap the phrase in double quotation marks: title:\"labour force survey\" . Specifying a date range \u00b6 Searching by specifying a date range can also be an effective way to find data. Range syntax is denoted in Abacus as [ TO ] , where both a lower and upper bound are specified. Different types of brackets indicate whether these bounds are inclusive or exclusive. Square brackets [] represent inclusive lower and upper bounds, whereas curly brackets {} represent exclusive bounds. These can be combined in a single query: {1956-02-16 TO 1988-05-03] . Dates in Abacus are formatted as YYYY-MM-DD , where: - YYYY is the year, - MM is the month, - DD is the day of the month, Truncation: Users do not need to specify complete dates and times to search using date ranges. For instance, the search pubdate:[1956 TO 1998-05] using the \"publication date\" field will produce relevant material from (and including) 1 January 1956 through 31 May 1998. Wildcard: You may use the * wildcard as the lower or upper bound. Entering [* TO 2001-09-11} would return all matched items from the earliest in the repository up to (but excluding) 11 September 2001, whereas entering [2001-09-11 TO *] would return all matched items from (and including) 11 September 2001 up to the most recent matched deposit.","title":"Search syntax tips"},{"location":"en/guides/user/search/#search-syntax-tips","text":"Boolean logical operators Search scope Querying specific fields Specifying a date range","title":"Search syntax tips"},{"location":"en/guides/user/search/#boolean-logical-operators","text":"Boolean logical operators greatly improve the power and efficiency of queries. By default, search terms are connected with the Boolean OR operator. Thus, the search adult literacy survey will match material containing \"adult\", \"literacy\", or \"survey\", but not necessarily all three. For results that contain all search terms, use the AND operator: adult AND literacy AND survey . In addition to the AND and OR operators, other useful Boolean operators include NOT , + , and - . The NOT and - operators function similarly, requiring that the term following the operator not be present in search results, while the + operator requires that the term is present. For instance, literacy NOT child , or literacy -child , will return all results containing \"literacy\" without the term \"child\", while +\"adult literacy\" survey returns results with the phrase \"adult literacy\" that may or may not contain the term \"survey\". In addition to their word form, Boolean operators can also be called in symbol form: Word Symbol AND && OR || NOT ! To use Boolean operators in word form (e.g. AND , OR , NOT ), all letters must be uppercase. Additionally, when using the ! operator, unlike its word form NOT , do not include a space before the term it applies to--correct: education !primary , incorrect: education ! primary . For added control over the Boolean logic of a search, users can group clauses to form sub-queries using parentheses. Searching for vaccine AND (\"corona virus\" OR \"COVID-19\") will return material that matches either \"vaccine\" and \"corona virus\" or \"vaccine\" and \"COVID-19\".","title":"Boolean logical operators"},{"location":"en/guides/user/search/#search-scope","text":"Abacus supports two distinct search scopes : \"global\" and \"local\". Using the search box on the left-hand side of the user interface will limit results to the dataverse currently selected (a \"local\" search), while using the search box through selecting the \"Search\" drop-down menu at the top of the user interface will execute a query on all dataverses in the Abacus Data Network (a \"global\" search).","title":"Search scope"},{"location":"en/guides/user/search/#querying-specific-fields","text":"Users can also narrow their search within Abacus by querying specific fields . The syntax for this requires, first, specifiying the field to be searched (e.g. title ), followed by a : , and then the term to search for: title : financial . (NOTE: There is no space between the colon and the search term. ) Searching for multiple terms within a field requires specifying the field before each term. For example, the sytnax for searching for the terms \"labour\", \"force\", and \"survey\" within the \"title\" field is title : labour AND title : force AND title : survey . Using the query title : labour force survey will search for \"labour\" in the \"title\" field and \"force\" and \"survey\" in any other metadata field. To search for the phrase \"labour force survey\" in the \"title\" field, wrap the phrase in double quotation marks: title:\"labour force survey\" .","title":"Querying specific fields"},{"location":"en/guides/user/search/#specifying-a-date-range","text":"Searching by specifying a date range can also be an effective way to find data. Range syntax is denoted in Abacus as [ TO ] , where both a lower and upper bound are specified. Different types of brackets indicate whether these bounds are inclusive or exclusive. Square brackets [] represent inclusive lower and upper bounds, whereas curly brackets {} represent exclusive bounds. These can be combined in a single query: {1956-02-16 TO 1988-05-03] . Dates in Abacus are formatted as YYYY-MM-DD , where: - YYYY is the year, - MM is the month, - DD is the day of the month, Truncation: Users do not need to specify complete dates and times to search using date ranges. For instance, the search pubdate:[1956 TO 1998-05] using the \"publication date\" field will produce relevant material from (and including) 1 January 1956 through 31 May 1998. Wildcard: You may use the * wildcard as the lower or upper bound. Entering [* TO 2001-09-11} would return all matched items from the earliest in the repository up to (but excluding) 11 September 2001, whereas entering [2001-09-11 TO *] would return all matched items from (and including) 11 September 2001 up to the most recent matched deposit.","title":"Specifying a date range"}]}